{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "texas tech google test",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJXrsPstyAEA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0EW0WB_pyaI"
      },
      "source": [
        "# INSTRUCTIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLzZsKoxM97S"
      },
      "source": [
        "Copy this notebook to your Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXmvX5uXajB"
      },
      "source": [
        "Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0S4wevQXY0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e3aa8f-7d13-472c-c89d-19132888d239"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEsOmGaI9Fpk"
      },
      "source": [
        "# general path:\n",
        "data_path = \"/content/gdrive/My Drive/data/\""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOC25sFyXulw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012e33a0-fced-4db6-b4f9-a44032d950de"
      },
      "source": [
        "# go to the folder:\n",
        "%cd /content/gdrive/My\\ Drive/data\n",
        "# print out the content of the folder:\n",
        "%ls"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/data\n",
            "\u001b[0m\u001b[01;34mFlowers\u001b[0m/       \u001b[01;34mMNIST\u001b[0m/          \u001b[01;34mmodels\u001b[0m/         \u001b[01;34mtxtdata\u001b[0m/\n",
            "\u001b[01;34mminiImageNet\u001b[0m/  mnist_model.pt  \u001b[01;34mtrain_subsets\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88jTsSCI8ah8"
      },
      "source": [
        "Now untar the files. This might take few minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuNMweurjuod"
      },
      "source": [
        "Confirm correct creation of 3 folders (train, test and val):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5f37CvljrJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc75d02-8cdb-48f0-9015-ad7c0d575393"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mFlowers\u001b[0m/       \u001b[01;34mMNIST\u001b[0m/          \u001b[01;34mmodels\u001b[0m/         \u001b[01;34mtxtdata\u001b[0m/\n",
            "\u001b[01;34mminiImageNet\u001b[0m/  mnist_model.pt  \u001b[01;34mtrain_subsets\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd5stBPKYDXE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7hWirg1yAEB"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqIfVxocyAED"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "train_dir = '/content/gdrive/My Drive/data/train_subsets'\n",
        "val_dir = '/content/gdrive/My Drive/data/train_subsets'\n",
        "model_dir = '/content/gdrive/My Drive/data/models'\n",
        "tf_dir = '/content/gdrive/My Drive/data/txtdata/trainingfiles/'\n",
        "vf_dir = '/content/gdrive/My Drive/data/txtdata/valfiles'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z1NH-E--Ap5"
      },
      "source": [
        "Create a custom dataset where you will use your text files to store information about your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9OFI9KO-IZb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "824c181a-3d91-4dcb-9a1d-09b858234109"
      },
      "source": [
        "# data transforms:\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([transforms.Resize((224,224)), transforms.RandomHorizontalFlip(),\n",
        "                  transforms.RandomRotation(20), transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0),\n",
        "                  transforms.ToTensor(), normalize])\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), normalize]) \n",
        "\n",
        "\n",
        "# Create your custom dataset below:\n",
        "class AntarcticPlotDataset(Dataset):\n",
        "    \n",
        "    #initializing the master array\n",
        "    newdata = []\n",
        "    \n",
        "    def __init__(self, txt_file, root_dir, transform=None):\n",
        "        \n",
        "        \n",
        "        #directory where all the images are\n",
        "        self.root_dir = root_dir\n",
        "        \n",
        "        #transforms to do on the images\n",
        "        self.transform = transform\n",
        "        \n",
        "        #init\n",
        "        self.newdata = []\n",
        "        \n",
        "        \n",
        "        #counter for next() method, no longer used\n",
        "        self.counter = -1\n",
        "        \n",
        "        #text file w/ target data for the images\n",
        "        photoData = txt_file\n",
        "        \n",
        "        \n",
        "        #reading the text file line by line \n",
        "        \n",
        "        for line in open(photoData):\n",
        "            print(line)\n",
        "            info = line\n",
        "\n",
        "    \n",
        "            infolist = info.split(\" \")\n",
        "            \n",
        "            #data from one line\n",
        "            finaldata = []\n",
        "            \n",
        "            #since all the sub images for one image rest in a master folder, need to get inside that folder\n",
        "            imgname = infolist[0]\n",
        "            \n",
        "            #seperating the .png from the image name\n",
        "            containingFolder = imgname.split(\"-\")[0]\n",
        "            \n",
        "            \n",
        "            #going into the folder named after the prent image\n",
        "            img_dir = os.path.join(root_dir, containingFolder)           \n",
        "            \n",
        "            #using the text file to get the name of the image\n",
        "            img = cv2.imread(os.path.join(img_dir, imgname))\n",
        "    \n",
        "            \n",
        "            #formatting and transforming the images\n",
        "            img = np.array(img)\n",
        "            trans = transforms.ToPILImage()\n",
        "            img = trans(img)\n",
        "            #print(np.asarray(img))\n",
        "            #print(img.size)\n",
        "            \n",
        "            \n",
        "            img = self.transform(img)\n",
        "\n",
        "            finaldata.append(img)\n",
        "            \n",
        "            \n",
        "            \n",
        "            #extracting the target data and adding it to the array for the line\n",
        "            finaldata.append(int(info.split(\":\")[1].strip('\\n')))\n",
        "            \n",
        "            \n",
        "            #adding the information from one line to the full master array\n",
        "            self.newdata.append(finaldata)\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "            \n",
        "                    \n",
        "    def __len__(self):\n",
        "        return len(self.newdata)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "         #gets the ith item in newdata\n",
        "        \n",
        "         #error check\n",
        "         if (i < 0 or i > len(self.newdata)):\n",
        "            print(\"problem\")\n",
        "            return None\n",
        "        \n",
        "         else:\n",
        "            \n",
        "            #extracting image      \n",
        "            img = self.newdata[i][0]\n",
        "            \n",
        "            #extracting target\n",
        "            landmarks = (self.newdata[i][1])\n",
        "            target = []\n",
        "            target.append(landmarks)\n",
        "            #storing both image in target in a dictionary format\n",
        "            sample = {'image' : img, 'landmarks' : landmarks}\n",
        "            #returning the dictionary - the enumerator wants a dictionary object\n",
        "            return sample\n",
        "\n",
        "\n",
        "\n",
        "train_rock = os.path.join(tf_dir, 'trainingfile_rock.txt')\n",
        "val_rock = os.path.join(vf_dir, 'valfile_rock.txt')\n",
        "\n",
        "train_data_rock = AntarcticPlotDataset(train_rock, train_dir, transform=train_transform)\n",
        "val_data_rock = AntarcticPlotDataset(val_rock, val_dir, transform=test_transform)\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10c_set4-1.JPG x:1\n",
            "\n",
            "10c_set4-10.JPG x:0\n",
            "\n",
            "10c_set4-11.JPG x:1\n",
            "\n",
            "10c_set4-12.JPG x:1\n",
            "\n",
            "10c_set4-13.JPG x:1\n",
            "\n",
            "10c_set4-14.JPG x:0\n",
            "\n",
            "10c_set4-15.JPG x:1\n",
            "\n",
            "10c_set4-16.JPG x:0\n",
            "\n",
            "10c_set4-17.JPG x:1\n",
            "\n",
            "10c_set4-18.JPG x:1\n",
            "\n",
            "10c_set4-19.JPG x:0\n",
            "\n",
            "10c_set4-2.JPG x:1\n",
            "\n",
            "10c_set4-20.JPG x:1\n",
            "\n",
            "10c_set4-21.JPG x:1\n",
            "\n",
            "10c_set4-22.JPG x:1\n",
            "\n",
            "10c_set4-23.JPG x:1\n",
            "\n",
            "10c_set4-24.JPG x:0\n",
            "\n",
            "10c_set4-25.JPG x:1\n",
            "\n",
            "10c_set4-26.JPG x:1\n",
            "\n",
            "10c_set4-27.JPG x:0\n",
            "\n",
            "10c_set4-28.JPG x:1\n",
            "\n",
            "10c_set4-29.JPG x:0\n",
            "\n",
            "10c_set4-3.JPG x:1\n",
            "\n",
            "10c_set4-30.JPG x:0\n",
            "\n",
            "10c_set4-31.JPG x:0\n",
            "\n",
            "10c_set4-32.JPG x:0\n",
            "\n",
            "10c_set4-33.JPG x:1\n",
            "\n",
            "10c_set4-34.JPG x:1\n",
            "\n",
            "10c_set4-35.JPG x:1\n",
            "\n",
            "10c_set4-36.JPG x:1\n",
            "\n",
            "10c_set4-37.JPG x:1\n",
            "\n",
            "10c_set4-38.JPG x:0\n",
            "\n",
            "10c_set4-39.JPG x:0\n",
            "\n",
            "10c_set4-4.JPG x:1\n",
            "\n",
            "10c_set4-40.JPG x:1\n",
            "\n",
            "10c_set4-41.JPG x:1\n",
            "\n",
            "10c_set4-42.JPG x:1\n",
            "\n",
            "10c_set4-43.JPG x:1\n",
            "\n",
            "10c_set4-44.JPG x:0\n",
            "\n",
            "10c_set4-5.JPG x:1\n",
            "\n",
            "10c_set4-6.JPG x:1\n",
            "\n",
            "10c_set4-7.JPG x:0\n",
            "\n",
            "10c_set4-8.JPG x:0\n",
            "\n",
            "10w_set5-1.JPG x:1\n",
            "\n",
            "10w_set5-10.JPG x:1\n",
            "\n",
            "10w_set5-11.JPG x:1\n",
            "\n",
            "10w_set5-12.JPG x:0\n",
            "\n",
            "10w_set5-13.JPG x:0\n",
            "\n",
            "10w_set5-14.JPG x:0\n",
            "\n",
            "10w_set5-15.JPG x:0\n",
            "\n",
            "10w_set5-16.JPG x:0\n",
            "\n",
            "10w_set5-17.JPG x:0\n",
            "\n",
            "10w_set5-18.JPG x:0\n",
            "\n",
            "10w_set5-19.JPG x:0\n",
            "\n",
            "10w_set5-2.JPG x:1\n",
            "\n",
            "10w_set5-20.JPG x:0\n",
            "\n",
            "10w_set5-21.JPG x:0\n",
            "\n",
            "10w_set5-22.JPG x:0\n",
            "\n",
            "10w_set5-23.JPG x:0\n",
            "\n",
            "10w_set5-24.JPG x:0\n",
            "\n",
            "10w_set5-25.JPG x:0\n",
            "\n",
            "10w_set5-26.JPG x:1\n",
            "\n",
            "10w_set5-27.JPG x:1\n",
            "\n",
            "10w_set5-28.JPG x:1\n",
            "\n",
            "10w_set5-29.JPG x:1\n",
            "\n",
            "10w_set5-3.JPG x:1\n",
            "\n",
            "10w_set5-30.JPG x:1\n",
            "\n",
            "10w_set5-31.JPG x:1\n",
            "\n",
            "10w_set5-32.JPG x:1\n",
            "\n",
            "10w_set5-33.JPG x:1\n",
            "\n",
            "10w_set5-34.JPG x:1\n",
            "\n",
            "10w_set5-35.JPG x:1\n",
            "\n",
            "10w_set5-36.JPG x:1\n",
            "\n",
            "10w_set5-37.JPG x:1\n",
            "\n",
            "10w_set5-38.JPG x:1\n",
            "\n",
            "10w_set5-39.JPG x:1\n",
            "\n",
            "10w_set5-4.JPG x:1\n",
            "\n",
            "10w_set5-40.JPG x:1\n",
            "\n",
            "10w_set5-41.JPG x:1\n",
            "\n",
            "10w_set5-42.JPG x:1\n",
            "\n",
            "10w_set5-43.JPG x:1\n",
            "\n",
            "10w_set5-44.JPG x:1\n",
            "\n",
            "10w_set5-45.JPG x:1\n",
            "\n",
            "10w_set5-46.JPG x:0\n",
            "\n",
            "10w_set5-47.JPG x:1\n",
            "\n",
            "10w_set5-5.JPG x:1\n",
            "\n",
            "10w_set5-6.JPG x:1\n",
            "\n",
            "10w_set5-7.JPG x:1\n",
            "\n",
            "10w_set5-8.JPG x:1\n",
            "\n",
            "10w_set5-9.JPG x:1\n",
            "\n",
            "11c_set5-1.JPG x:0\n",
            "\n",
            "11c_set5-10.JPG x:0\n",
            "\n",
            "11c_set5-11.JPG x:0\n",
            "\n",
            "11c_set5-12.JPG x:0\n",
            "\n",
            "11c_set5-13.JPG x:0\n",
            "\n",
            "11c_set5-14.JPG x:0\n",
            "\n",
            "11c_set5-15.JPG x:0\n",
            "\n",
            "11c_set5-16.JPG x:0\n",
            "\n",
            "11c_set5-17.JPG x:0\n",
            "\n",
            "11c_set5-18.JPG x:0\n",
            "\n",
            "11c_set5-19.JPG x:0\n",
            "\n",
            "11c_set5-2.JPG x:0\n",
            "\n",
            "11c_set5-20.JPG x:0\n",
            "\n",
            "11c_Set5-21.JPG x:0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-fb5178158b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mval_rock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valfile_rock.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mtrain_data_rock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAntarcticPlotDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0mval_data_rock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAntarcticPlotDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_rock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-fb5178158b48>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, txt_file, root_dir, transform)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;31m#print(np.asarray(img))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m#print(img.size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be 2/3 dimensional. Got {} dimensions.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 0 dimensions."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYh5DfWX_DdT"
      },
      "source": [
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0 # means to use all\n",
        "# how many samples per batch to load\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tl_rock = torch.utils.data.DataLoader(train_data_rock, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_rock = torch.utils.data.DataLoader(val_data_rock,  num_workers = 0, batch_size=batch_size)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TighTVKsyAEJ"
      },
      "source": [
        "Define the network:\n",
        "\n",
        "We will use an existing network. Copy the ConvNet.py file from [here](https://drive.google.com/file/d/1Hf6j95u88qwJM3TnsWeiKGLTxaFHh4h4/view?usp=sharing) and put it in your \"miniImageNet\" folder on your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd547-kayAEJ"
      },
      "source": [
        "class AntarcticNet(nn.Module):\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super(AntarcticNet, self).__init__()\n",
        "\n",
        "        #planning for 3+1 stacks \n",
        "        \n",
        "        #bias is default true\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size = 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(10, 16, kernel_size = 3, stride=1, padding=1)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(16, 24, kernel_size = 3,stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(24, 32, kernel_size = 3, stride=1, padding=1)\n",
        "        \n",
        "        self.conv5 = nn.Conv2d(32, 44, kernel_size = 3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(44, 64, kernel_size = 3, stride=1, padding=1)\n",
        "        \n",
        "        #for stack 4\n",
        "        \n",
        "        self.conv7 = nn.Conv2d(64, 81, kernel_size = 3, stride=1, padding=1)\n",
        "        self.conv8 = nn.Conv2d(81, 96, kernel_size = 3, stride =1, padding=1)\n",
        "        \n",
        "        #for stack 5\n",
        "        \n",
        "        self.conv9 = nn.Conv2d(96, 128, kernel_size = 3, stride=1, padding=1)\n",
        "        self.conv10 = nn.Conv2d(128, 128, kernel_size = 3, stride =1, padding=1)\n",
        "        \n",
        "        \n",
        "        # lin conversion\n",
        "        \n",
        "        \n",
        "        self.lin = nn.Linear(in_features=6272, out_features=128)\n",
        "        self.lin2 = nn.Linear(in_features = 128, out_features=1)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(kernel_size = 3, stride=(2,2), padding=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #stack 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(self.pool(x))\n",
        "        \n",
        "        \n",
        "        #stack 2\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = torch.relu(self.pool(x))\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        #stack 3\n",
        "        \n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = torch.relu(self.pool(x))\n",
        "\n",
        "\n",
        "\n",
        "        #stack 4\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = torch.relu(self.pool(x))\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        #stack 5\n",
        "        \n",
        "        x = self.conv9(x)\n",
        "        x = torch.relu(self.pool(x)) #last pooling layer\n",
        "\n",
        "        \n",
        "        \n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        x = torch.relu(self.lin(x))\n",
        "        return self.lin2(x)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhRDTUgGUfVK"
      },
      "source": [
        "Now let's specify the loss function that we will use.\n",
        "For this reason let's adapt Contrastive Loss implemented [here](https://github.com/adambielski/siamese-triplet/blob/master/losses.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pMld9zBfEz3"
      },
      "source": [
        "Let's create a function that will take our model as a parameter and will choose n images from each category and generate their embeddings (extract feature vectors that describe the images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT18rNBA96i-"
      },
      "source": [
        "Let's read the information about the validation and test splits and store it in two lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQy7FBy-CNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301e619d-b5f1-449f-a554-1038fa06176d"
      },
      "source": [
        "rock_model = AntarcticNet()\n",
        "\n",
        "\n",
        "# specify loss function\n",
        "#criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = nn.MSELoss()\n",
        "#criterion = nn.BCELoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.SGD(rock_model.parameters(), lr= 0.01, momentum=0.9)\n",
        "\n",
        "# specify scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'rock_model.pt')\n",
        "torch.save(rock_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_epochs = 2\n",
        "n_iterations = int(len(train_data_rock)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []\n",
        "\n",
        "\n",
        "rock_model.train()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AntarcticNet(\n",
              "  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5): Conv2d(32, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv6): Conv2d(44, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv7): Conv2d(64, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv8): Conv2d(81, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv9): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (lin): Linear(in_features=6272, out_features=128, bias=True)\n",
              "  (lin2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (pool): MaxPool2d(kernel_size=3, stride=(2, 2), padding=1, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXpAwuHy-De8"
      },
      "source": [
        "Extract embeddings:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOg4ig-hfpuu"
      },
      "source": [
        "Let's create a testing routine to predict samples in the val/test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBpytd08yAEh"
      },
      "source": [
        "Training the network.\n",
        "\n",
        "We will iterate through our dataset. For evey iteration we need to:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhWP7P7CyAEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0955a6f5-7a98-42f8-9720-1c71b72a8e0c"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    rock_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_rock):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = rock_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_rock.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    rock_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_rock):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = rock_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(rock_model, PATH)\n",
        "\n",
        "    "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.362067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the validation set: 41 %\n",
            "Epoch: 2 \tTraining Loss: 0.354061\n",
            "Accuracy of the network on the validation set: 41 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2kZs54yGNeY"
      },
      "source": [
        "After training, test the model on the test set. Write the code for it below. Use the example of validation set as your reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_4KVbb-GePi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "dcf8ae37-26c2-441d-976e-54c401117e14"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hU1bnH8e9rwh0ETagCAQMWkDtoEEVREFEEqmCrtvVebxwVtFZBPa312Noq56m1VK1aL2hbK6cgXkChIqJYqhAQRcUL4i2IGoKGWwME3vPHmjCRCSGQmewk8/s8zzzM7NkzeVfCM79Za++9lrk7IiIi5e0XdQEiIlL7KBxERCSBwkFERBIoHEREJIHCQUREEmRGXUAyZGdne25ubtRliIjUKUuWLFnr7q0req5ehENubi75+flRlyEiUqeY2Se7e07DSiIikiCtw2H7dtA1gCIiidI6HKZMgUMPhXHjYM4cKCmJuiIRkdqhXhxz2Fft20PPnvDgg3DXXdC0KQwbBiNHhlvbtlFXKJLetm3bRkFBASX65lYtjRs3JicnhwYNGlT5NVYf5lbKy8vz6hyQ/s9/YP58mDkTZs2CT2KHaPr1g1Gjwi0vD/ZL636WSM376KOPaNGiBVlZWZhZ1OXUSe5OUVERGzZsoGPHjt96zsyWuHteRa/Txx3QpAmccgrcfTd89BEsXw633QbNmsGtt8KAAdCmDVx4IUybBuvXR12xSHooKSlRMFSTmZGVlbXXva+0HlaqiFkYaurZEyZOhKKicDxi5kx46qlwnKJBAzjuuDD0NGoUdO4cddUi9ZeCofr25XeonsMeZGXBj38Mjz0GX30FL78M11wDX3wR/u3SJdyuuQZeeAG2bo26YhGR6lM47IXMTBg0KAw5vfVWGIK66y747nfhnnvgxBMhOxt+8IPQw/jyy6grFpHqKCoqom/fvvTt25eDDz6Ydu3a7Xy8dQ/fBPPz8xk/fvxe/bzc3FzWrl1bnZKTRsNK1ZCbC1dcEW6bNoWew6xZYQhq+vQwRNW/fxh6GjkyHOBWD1mk7sjKymLZsmUA3HzzzTRv3pxrr7125/OlpaVkZlb8MZqXl0deXoXHeusE9RySpFkzOPVUuO8+KCiA11+HW24JYfDLX8IRR0BODlxySTh2sWlT1BWLyL644IILGDt2LAMGDGDChAksWrSIo48+mn79+jFw4EDee+89AObPn8+oUaOAECw/+clPGDx4MJ06dWLy5Ml7/Dl33HEHPXv2pGfPntx5550AbNq0iZEjR9KnTx969uzJ1KlTAbj++uvp3r07vXv3/lZ4VYd6DilgBn37htvPfx6OVTz3XOhV/N//wQMPQKNGMHhwvFexyxlmIrKLq6+G2Jf4pOnbF2Kfu3uloKCAhQsXkpGRwfr161mwYAGZmZnMnTuXG2+8kenTpye85t133+XFF19kw4YNdO3alf/6r//a7XUHS5Ys4eGHH+a1117D3RkwYADHH388q1atom3btsyaNQuA4uJiioqKmDFjBu+++y5mxjfffLP3DaqAeg414DvfgfPPD8FQWBiGn664IhyzGDcOOnWCHj1gwoRwwLu0NOqKRaQyZ5xxBhkZGUD4gD7jjDPo2bMnP/3pT3n77bcrfM3IkSNp1KgR2dnZfOc73+HLSg5KvvLKK4wZM4ZmzZrRvHlzTj/9dBYsWECvXr14/vnnmThxIgsWLKBly5a0bNmSxo0bc9FFF/HEE0/QtGnTpLRRPYca1rAhnHBCuP3ud/DBB6FHMWtW+Abzv/8LrVrB8OGhVzF8eDhjSiTd7cs3/FRp1qzZzvu/+MUvGDJkCDNmzODjjz9m8ODBFb6mUaNGO+9nZGRQug/fArt06cLSpUt59tln+fnPf87QoUO56aabWLRoES+88ALTpk3jrrvuYt68eXv93rtSzyFinTuH7vLzz8PateFA9pgxMG8enHNO6HUceyz89rfh4rx6cEG7SL1SXFxMu3btAJgyZUpS3nPQoEE8+eSTbN68mU2bNjFjxgwGDRrE559/TtOmTTnnnHO47rrrWLp0KRs3bqS4uJgRI0bw+9//njfeeCMpNajnUIvsvz+cfnq47dgBS5bEp/S48cZw69AhfvHdkCHh6m4Ric6ECRM4//zz+fWvf83IkSOT8p6HH344F1xwAUceeSQAF198Mf369WPOnDlcd9117LfffjRo0IA//elPbNiwgdNOO42SkhLcnTvuuCMpNWhupTri88/h2WdDUDz/fDjbqUkTGDo0flA7JyfqKkWSa8WKFXTr1i3qMuqFin6Xlc2tpJ5DHdG2LVx8cbht2QIvvRR6FWU3gD594r2KI4+E2PEyEZG9pmMOdVCjRnDSSTB5Mnz4IbzzDkyaFA5k3347DBwIBx8M550HU6dCks5sE5E0op5DHWcG3bqF23XXwddfwz//GXoTzz4Lf/lL6EEMGhTvVXTtqiu1pe5wd02+V037cvhAPYd65oAD4KyzQih8+SX8619hdtl160J4dOsW5oK66qoQIlu2RF2xyO41btyYoqKiffpwk6BsPYfGjRvv1et0QDqNfPpp6E3MnBkuxCspCdN+nHRS6FWMGBHWrRCpLbQSXHLsbiW4yg5IKxzS1ObN8OKL8VNlP/ssbD/iiPjZT0ccodXvROqzaq8EZ2bDzew9M1tpZtdX8PxYM1tuZsvM7BUz617uud5m9m8zezu2T+PY9iNij1ea2WSLDSqa2YFm9ryZfRD794B9a7ZUpmnTEAB/+lNYFvWNN+A3vwkHu2+5JZzt1LYtXHQRPPEEbNgQdcUiUpP22HMwswzgfWAYUAAsBn7k7u+U22d/d18fu38qcLm7DzezTGApcK67v2FmWcA37r7dzBYB44HXgGeBye7+nJlNAta5+22xIDrA3SdWVqN6Dsm1di3Mnh16FbNnQ3FxWP3u+OPjvYrvfjfqKkWkuqrbczgSWOnuq9x9K/A4cFr5HcqCIaYZUJY4JwFvuvsbsf2KYsHQBtjf3V/1kE6PAqNjrzkNeCR2/5Fy26WGZGeHqTsefzxMFDh/fpjiY/Xq8G/nznDYYXDttWFoatu2qCsWkWSrSji0Az4r97ggtu1bzOwKM/sQmEToEQB0AdzM5pjZUjObUO49C3bznge5+5rY/S+AgyoqyswuNbN8M8svLCysQjNkX5T1GCZNCtdTfPhhuL7ikEPgj38MEwhmZ8OZZ8Kjj4YwEZG6L2mHG939bnc/FJgI/Dy2ORM4Fjg79u8YMxu6F+/pxHshuz53v7vnuXte69atq1e8VFmnTmGa8TlzoKgIZswIwfDKK2Fa8oMOgqOPhltvDXPv14PzHUTSUlXCYTXQvtzjnNi23Xmc+FBQAfCyu691982EYwuHx15ffiag8u/5ZWzYidi/X1WhRolA8+YwejT8+c9h9bslS+Dmm2H79rDIUb9+0L49XHYZPPNMOENKROqGqoTDYqCzmXU0s4bAD4Gny+9gZp3LPRwJfBC7PwfoZWZNYwenjwfeiQ0brTezo2JnKZ0HPBV7zdPA+bH755fbLrXYfvvB4YfDTTfBokWwZg089BAcdRQ89lhYQvXAA8O1FPfcE86QEpHaq0rXOZjZCOBOIAN4yN1vNbNbgHx3f9rM/gCcCGwDvgaudPe3Y689B7iBMDz0rLtPiG3PA6YATYDngHHu7rEzmv4P6AB8Apzp7usqq09nK9VuW7eGFe5mzQpnQK1cGbb37Bmf0uOoo2A367SLSIroIjipVd5/P37xXdmyqAccAKecEsJi+PDQyxCR1FI4SK1VXBzWpyibKLCwMAxRHXNMvFfRvbsmChRJBYWD1Ak7dsDixfFexeuvh+2HHBJCYtQoGDwY9nL+MBHZDYWD1EkFBfHV7+bODWc7NW0KJ54YehUjR0K7hCtuRKSqFA5S55WUhCu1y1a+KzvbqW/feK+if39NFCiyNxQOUq+4h6u1y4af/vWvMCTVunU4VXbkyDANecuWUVcqUrspHKReW7cuXLE9cyY891xYDS8zM6x+V9ar6NIl6ipFah+Fg6SN0lJ49dV4r+Ktt8L27343PqPsccdBw4bR1ilSGygcJG198kn84rt588KyqC1awLBhISxGjAjzQYmkI4WDCLBpUwiIsl7F6thsXv37x3sV/frpoLakD4WDyC7cw+p3Zb2K114L29q0Cb2JUaPCKbPNm0ddqUjqKBxE9qCwMBzMnjkzHNxevz4clxg8ON6r6NQp6ipFkkvhILIXtm0L61OUDT+9917Y3q1bfEqPgQPDQkgidZnCQaQaVq6MDz+99FIIj1at4OSTQ1AMHx5WwxOpaxQOIkmyYUOYKHDWrHD78stwAPuoo+LDT716aaJAqRsUDiIpsGNHWP2urFexZEnY3r59fPhpyJAwH5RIbaRwEKkBa9aEiQJnzgy9i02bwgyyQ4fGJwrs0CHqKkXiFA4iNWzLlnB8oqxXsWpV2N67d7xXMWAAZGREW6ekN4WDSITcwxlPZTPKvvIKbN8OWVnx1e9OPjmshidSkxQOIrXIN9+EaylmzQrDUEVFoQdx7LHxXsVhh+mgtqReZeFQpYkCzGy4mb1nZivN7PoKnh9rZsvNbJmZvWJm3WPbc83sP7Hty8zs3tj2FuW2LTOztWZ2Z+y5C8yssNxzF+9700Vqn1at4Kyz4NFHw9lOCxfCxIkhNCZMCMuiHnoojB8fQqSkJOqKJR3tsedgZhnA+8AwoABYDPzI3d8pt8/+7r4+dv9U4HJ3H25mucBMd++5h5+xBPipu79sZhcAee5+ZVUboZ6D1BeffRY/TXbu3BAMzZqFqTzKJgps2zbqKqW+qG7P4UhgpbuvcvetwOPAaeV3KAuGmGZAlceqzKwL8B1gQVVfI1JftW8PY8fCM8+EdSpmzYLzzoOlS+GSS8KyqEccAb/8JSxaFE6nFUmFqoRDO+Czco8LYtu+xcyuMLMPgUnA+HJPdTSz183sJTMbVMH7/xCY6t/uwnzfzN40s2lm1r6ioszsUjPLN7P8wsLCKjRDpG5p0iT0FO65J0w9/uab8JvfhO2//nU426ltW/jJT2D69DAflEiyVGVY6QfAcHe/OPb4XGDA7oZ9zOzHwMnufr6ZNQKau3uRmR0BPAn0KN/TMLN3gHPdfUnscRaw0d23mNllwFnufkJlNWpYSdLN2rUwe3boWcyeHY5XNGgQFjIqu1K7c+eoq5TarrrDSquB8t/ec2LbdudxYDSAu29x96LY/SXAh8DOBRvNrA+QWRYMsf2K3H1L7OEDwBFVqFEkrWRnwznnwN//HmaUfekluPrqcCHeT38alkXt2hWuuSasYbF1a9QVS11TlXBYDHQ2s45m1pAwDPR0+R3MrPx3lJHAB7HtrWMHtDGzTkBnYFW5fX8E/H2X92pT7uGpwIqqNUUkPWVmhh7DpEnw9tvhgrs//hE6doS77w5XaLduDWecAY88Al99FXXFUhdk7mkHdy81syuBOUAG8JC7v21mtwD57v40cKWZnQhsA74Gzo+9/DjgFjPbBuwAxrr7unJvfyYwYpcfOT52xlMpsA64YJ9bJ5KGOnaEK68Mt40b4YUX4tOPT5sWrp848sj48FPfvrqmQhLpIjiRNOEOr78en9Jj0aKwvV27+Op3Q4eGU2clPegKaRFJ8OWX8dXv/vnPMB15o0ZhJtmyK7Vzc6OuUlJJ4SAildq6FRYsiM//tHJl2N6jRzwojj46HN+Q+kPhICJ75f3348NPL78MpaVhYsDhw+Or3x14YNRVSnUpHERknxUXf3v1u8LCsPrdwIHxXkWPHjqoXRcpHEQkKXbsgMWL472K118P2zt0CCExahQMHhyu4pbaT+EgIimxenV89bu5c2Hz5hAMJ54YX/0uJyfqKmV3FA4iknIlJTB/frxX8fHHYXufPvFrKo48Uqvf1SYKBxGpUe7wzjvxoFi4MKx+l50drqkoW/2uZcuoK01vCgcRidS6dfHV7557LjzOzAyr35Udq+jSRQe1a5rCQURqjdJSePXVeK/irbfC9kMPjQ8/HXdcuCBPUkvhICK11iefxE+TfeEF2LIFmjeHYcPiq98dfHDUVdZPCgcRqRM2bw5TjJddqb06tjhAXl68V3H44eE6C6k+hYOI1DnuYfW7shllX301bDv44PhEgSeeCC1aRF1p3aVwEJE6r7AwHMwuW/1u/Xpo2BCOPz7eqzj00KirrFsUDiJSr2zbBv/6V3z46b33wvbDDotP6XHMMWHpVNk9hYOI1GsrV8YPas+fH8KjZctwLcWoUXDKKeEaC/k2hYOIpI0NG8JUHjNnhqk9vvgiXD9x1FHxXkXv3rqmAhQOIpKmduyApUvjB7XLPiZycuJBccIJ0LRptHVGReEgIgKsWfPt1e82bYLGjUNAlE0UeMghUVdZcyoLhyqdLWxmw83sPTNbaWbXV/D8WDNbbmbLzOwVM+se255rZv+JbV9mZveWe8382HuWPfed2PZGZjY19rNeM7PcfWm0iMiu2rSBn/wEnngCiopCQFx6Kbz7LlxxRVgWtXdvuOGGcMB7+/aoK47OHnsOZpYBvA8MAwqAxcCP3P2dcvvs7+7rY/dPBS539+GxD/aZ7t6zgvedD1zr7vm7bL8c6O3uY83sh8AYdz+rshrVcxCR6nAPZzyVTemxYEEIhgMPDAezR44Mq98dcEDUlSZXdXsORwIr3X2Vu28FHgdOK79DWTDENAOqM1Z1GvBI7P40YKiZDh2JSOqYhdNgf/YzePFFWLsWpk4NoTBnDvz4x9C6dZjzadKkMONsPRiRr1RVwqEd8Fm5xwWxbd9iZleY2YfAJGB8uac6mtnrZvaSmQ3a5WUPx4aUflEuAHb+PHcvBYqBrAp+3qVmlm9m+YWFhVVohohI1bRqBWeeCY8+Gs52WrgQrr8+XHg3cWJYFrVTJxg3LlyQV1ISdcXJl7QZStz9bnc/FJgI/Dy2eQ3Qwd37AdcAj5nZ/rHnznb3XsCg2O3cvfx597t7nrvntW7dOjmNEBHZRUYGHH00/PrXsGwZfPop3Hsv9OwJDz4Yhp2ysmD0aPjzn+Hzz6OuODmqEg6rgfblHufEtu3O48BoAHff4u5FsftLgA+BLrHHq2P/bgAeIwxffevnmVkm0BIoqlpzRERSq317uOwyeOaZcFB71iy44IKwnvall0K7dmFywJtugtdeC6fT1kVVCYfFQGcz62hmDYEfAk+X38HMOpd7OBL4ILa9deyANmbWCegMrDKzTDPLjm1vAIwCYrO68zRwfuz+D4B5Xh/OtxWReqdJkzAJ4N13h2VR33wTfvvbcN3ErbeGC+/atAnhMW1aGJaqKzL3tIO7l5rZlcAcIAN4yN3fNrNbgHx3fxq40sxOBLYBXxP/cD8OuMXMtgE7gLHuvs7MmgFzYsGQAcwF/hx7zYPAX8xsJbCOEEYiIrWaGfTqFW7XXx96FbNnh7OfnnoKHnkkrH533HHxiQK7dIm66t3TRXAiIilWWhoOapedKvtO7EKAzp3jQTFoUJhltibpCmkRkVrko4/iEwXOmwdbt4Z1KU46KT5R4EEHpb4OhYOISC21cWNYHrUsLD7/PAxR9e8fn/+pX7/UTBSocBARqQPcw+myZRMFLloUtrVtG1/9bujQsMZ2MigcRETqoC+/jK9+N2dOmI68YUMYMiR+rKJjx31//2pPvCciIjXvoIPCabD/+EeY0uOFF8IEgatWhauzO3WCP/whNT97j6eyiohI9Bo2DFOLn3AC3HEHvP9+6FGcdFJqfp7CQUSkDurSJbXXSWhYSUREEigcREQkQb04W8nMCoFP9vHl2cDaJJZTF6jN6UFtTg/VafMh7l7htNb1Ihyqw8zyd3cqV32lNqcHtTk9pKrNGlYSEZEECgcREUmgcID7oy4gAmpzelCb00NK2pz2xxxERCSReg4iIpJA4SAiIgnSIhzM7CEz+8rM3trN82Zmk81spZm9aWaH13SNyVaFNp8da+tyM1toZn1qusZk21Oby+3X38xKzewHNVVbqlSlzWY22MyWmdnbZvZSTdaXClX4v93SzJ4xszdibb6wpmtMJjNrb2Yvmtk7sfZcVcE+Sf8MS4twAKYAwyt5/hSgc+x2KfCnGqgp1aZQeZs/Ao53917Ar6gfB/KmUHmbMbMM4HbgnzVRUA2YQiVtNrNWwD3Aqe7eAzijhupKpSlU/ne+AnjH3fsAg4HfmVkNL8CZVKXAz9y9O3AUcIWZdd9ln6R/hqVFOLj7y8C6SnY5DXjUg1eBVmbWpmaqS409tdndF7r717GHrwI5NVJYClXh7wwwDpgOfJX6ilKvCm3+MfCEu38a27/Ot7sKbXaghZkZ0Dy2b2lN1JYK7r7G3ZfG7m8AVgDtdtkt6Z9haREOVdAO+Kzc4wISf/n12UXAc1EXkWpm1g4YQ/3oGVZVF+AAM5tvZkvM7LyoC6oBdwHdgM+B5cBV7r4j2pKSw8xygX7Aa7s8lfTPME3ZnebMbAghHI6NupYacCcw0d13WCoW5K2dMoEjgKFAE+DfZvaqu78fbVkpdTKwDDgBOBR43swWuPv6aMuqHjNrTuj1Xl0TbVE4BKuB9uUe58S21Wtm1ht4ADjF3YuirqcG5AGPx4IhGxhhZqXu/mS0ZaVUAVDk7puATWb2MtAHqM/hcCFwm4eLuFaa2UfAYcCiaMvad2bWgBAMf3P3JyrYJemfYRpWCp4Gzosd8T8KKHb3NVEXlUpm1gF4Aji3nn+L3MndO7p7rrvnAtOAy+t5MAA8BRxrZplm1hQYQBizrs8+JfSUMLODgK7AqkgrqobYsZMHgRXufsdudkv6Z1ha9BzM7O+EsxayzawA+CXQAMDd7wWeBUYAK4HNhG8edVoV2nwTkAXcE/smXVrXZ7OsQpvrnT212d1XmNls4E1gB/CAu1d6qm9tV4W/86+AKWa2HDDCUGJdnsb7GOBcYLmZLYttuxHoAKn7DNP0GSIikkDDSiIikkDhICIiCRQOIiKSoF4ckM7Ozvbc3NyoyxARqVOWLFmydndrSNeLcMjNzSU/Pz/qMkRE6hQz+2R3z2lYSUREEigcREQkgcJBREQS1ItjDiISvW3btlFQUEBJSUnUpcguGjduTE5ODg0aNKjyaxQOIpIUBQUFtGjRgtzcXNJo1ttaz90pKiqioKCAjh07Vvl1GlYSkaQoKSkhKytLwVDLmBlZWVl73aNTOIhI0igYaqd9+bsoHEREJIGOOYhIvVBUVMTQoUMB+OKLL8jIyKB163Dx76JFi2jYsGGlr58/fz4NGzZk4MCBKa+1LlA4iEi9kJWVxbJlYbmDm2++mebNm3PttddW+fXz58+nefPmkYfD9u3bycjIiLQGUDiISCpcfTUsW7bn/fZG375w55179ZIlS5ZwzTXXsHHjRrKzs5kyZQpt2rRh8uTJ3HvvvWRmZtK9e3duu+027r33XjIyMvjrX//KH//4RwYNGrTzfRYtWsRVV11FSUkJTZo04eGHH6Zr165s376diRMnMnv2bPbbbz8uueQSxo0bx+LFi7nqqqvYtGkTjRo14oUXXmD69Onk5+dz1113ATBq1CiuvfZaBg8eTPPmzbnsssuYO3cud999N/PmzeOZZ57hP//5DwMHDuS+++7DzFi5ciVjx46lsLCQjIwM/vGPf/A///M/nH766YwePRqAs88+mzPPPJPTTjutWr9uhYOI1Evuzrhx43jqqado3bo1U6dO5b//+7956KGHuO222/joo49o1KgR33zzDa1atWLs2LG77W0cdthhLFiwgMzMTObOncuNN97I9OnTuf/++/n4449ZtmwZmZmZrFu3jq1bt3LWWWcxdepU+vfvz/r162nSpEmltW7atIkBAwbwu9/9DoDu3btz0003AXDuuecyc+ZMvve973H22Wdz/fXXM2bMGEpKStixYwcXXXQRv//97xk9ejTFxcUsXLiQRx55pNq/P4WDiCTfXn7DT4UtW7bw1ltvMWzYMCAM17Rp0waA3r17c/bZZzN69Oid37grU1xczPnnn88HH3yAmbFt2zYA5s6dy9ixY8nMDB+lBx54IMuXL6dNmzb0798fgP3333+P75+RkcH3v//9nY9ffPFFJk2axObNm1m3bh09evRg8ODBrF69mjFjxgDhwjaA448/nssvv5zCwkKmT5/O97///Z31VIfCQUTqJXenR48e/Pvf/054btasWbz88ss888wz3HrrrSxfvrzS9/rFL37BkCFDmDFjBh9//DGDBw/e63oyMzPZsWPHzsflrzto3LjxzuMMJSUlXH755eTn59O+fXtuvvnmPV6jcN555/HXv/6Vxx9/nIcffniva6uITmUVkXqpUaNGFBYW7gyHbdu28fbbb7Njxw4+++wzhgwZwu23305xcTEbN26kRYsWbNiwocL3Ki4upl27dgBMmTJl5/Zhw4Zx3333UVpaCsC6devo2rUra9asYfHixQBs2LCB0tJScnNzWbZs2c6fv2jRogp/VlkQZGdns3HjRqZNmwZAixYtyMnJ4cknnwRCz2jz5s0AXHDBBdwZ66117959n39n5SkcRKRe2m+//Zg2bRoTJ06kT58+9O3bl4ULF7J9+3bOOeccevXqRb9+/Rg/fjytWrXie9/7HjNmzKBv374sWLDgW+81YcIEbrjhBvr167czCAAuvvhiOnToQO/evenTpw+PPfYYDRs2ZOrUqYwbN44+ffowbNgwSkpKOOaYY+jYsSPdu3dn/PjxHH744RXW3apVKy655BJ69uzJySefvHN4CuAvf/kLkydPpnfv3gwcOJAvvvgCgIMOOohu3bpx4YUXJu33Z+6etDeLSl5enmuxH5ForVixgm7dukVdRlravHkzvXr1YunSpbRs2bLCfSr6+5jZEnfPq2h/9RxEROqwuXPn0q1bN8aNG7fbYNgXOiAtIlKHnXjiiXzyyW5X+9xn6jmISNLUh2Hq+mhf/i4KBxFJisaNG1NUVKSAqGXK1nMouy6iqjSsJCJJkZOTQ0FBAYWFhVGXIrsoWwlubygcRCQpGjRosFcrjUntpmElERFJoHAQEZEECgcREUmgcBARkQQKBxERSaBwEBGRBAoHESI+zA0AAAbMSURBVBFJoHAQEZEECgcREUkQeTiYWYaZvW5mM2OP/2Zm75nZW2b2kJk1iLpGEZF0E3k4AFcBK8o9/htwGNALaAJcHEVRIiLpLNJwMLMcYCTwQNk2d3/WY4BFwN7NFiUiItUWdc/hTmACsGPXJ2LDSecCsyt6oZldamb5ZpavWSBFRJIrsnAws1HAV+6+ZDe73AO87O4LKnrS3e939zx3z2vdunXK6hQRSUdRTtl9DHCqmY0AGgP7m9lf3f0cM/sl0Bq4LML6RETSVmQ9B3e/wd1z3D0X+CEwLxYMFwMnAz9y94ThJhERSb2ojzlU5F7gIODfZrbMzG6KuiARkXRTK1aCc/f5wPzY/VpRk4hIOquNPQcREYmYwkFERBIoHEREJIHCQUREEigcREQkQXqfGXT11bBsWdRViIjsu7594c47k/626jmIiEiC9O45pCBtRUTqA/UcREQkgcJBREQSKBxERCSBwkFERBIoHEREJIHCQUREEigcREQkgcJBREQSKBxERCSBwkFERBIoHEREJIHCQUREEigcREQkgcJBREQSKBxERCSBwkFERBIoHEREJIHCQUREEigcREQkgcJBREQSKBxERCSBwkFERBIoHEREJIHCQUREEigcREQkgcJBREQSRB4OZpZhZq+b2czY445m9pqZrTSzqWbWMOoaRUTSTeThAFwFrCj3+Hbg9+7+XeBr4KJIqhIRSWORhoOZ5QAjgQdijw04AZgW2+URYHQ01YmIpK+oew53AhOAHbHHWcA37l4ae1wAtKvohWZ2qZnlm1l+YWFh6isVEUkjkYWDmY0CvnL3Jfvyene/393z3D2vdevWSa5ORCS9ZUb4s48BTjWzEUBjYH/gD0ArM8uM9R5ygNUR1igikpYi6zm4+w3unuPuucAPgXnufjbwIvCD2G7nA09FVKKISNqK+phDRSYC15jZSsIxiAcjrkdEJO1EOay0k7vPB+bH7q8CjoyyHhGRdFcbew4iIhIxhYOIiCRQOIiISAKFg4iIJFA4iIhIAoWDiIgkUDiIiEgChYOIiCRQOIiISAKFg4iIJFA4iIhIAoWDiIgkUDiIiEgCc/eoa6g2MysEPtnHl2cDa5NYTl2gNqcHtTk9VKfNh7h7hUtp1otwqA4zy3f3vKjrqElqc3pQm9NDqtqsYSUREUmgcBARkQQKB7g/6gIioDanB7U5PaSkzWl/zEFERBKp5yAiIgkUDiIikiAtwsHMHjKzr8zsrd08b2Y22cxWmtmbZnZ4TdeYbFVo89mxti43s4Vm1qema0y2PbW53H79zazUzH5QU7WlSlXabGaDzWyZmb1tZi/VZH2pUIX/2y3N7BkzeyPW5gtrusZkMrP2Zvaimb0Ta89VFeyT9M+wtAgHYAowvJLnTwE6x26XAn+qgZpSbQqVt/kj4Hh37wX8ivpxIG8KlbcZM8sAbgf+WRMF1YApVNJmM2sF3AOc6u49gDNqqK5UmkLlf+crgHfcvQ8wGPidmTWsgbpSpRT4mbt3B44CrjCz7rvsk/TPsLQIB3d/GVhXyS6nAY968CrQysza1Ex1qbGnNrv7Qnf/OvbwVSCnRgpLoSr8nQHGAdOBr1JfUepVoc0/Bp5w909j+9f5dlehzQ60MDMDmsf2La2J2lLB3de4+9LY/Q3ACqDdLrsl/TMsLcKhCtoBn5V7XEDiL78+uwh4LuoiUs3M2gFjqB89w6rqAhxgZvPNbImZnRd1QTXgLqAb8DmwHLjK3XdEW1JymFku0A94bZenkv4ZllmdF0vdZ2ZDCOFwbNS11IA7gYnuviN8qUwLmcARwFCgCfBvM3vV3d+PtqyUOhlYBpwAHAo8b2YL3H19tGVVj5k1J/R6r66JtigcgtVA+3KPc2Lb6jUz6w08AJzi7kVR11MD8oDHY8GQDYwws1J3fzLaslKqAChy903AJjN7GegD1OdwuBC4zcNFXCvN7CPgMGBRtGXtOzNrQAiGv7n7ExXskvTPMA0rBU8D58WO+B8FFLv7mqiLSiUz6wA8AZxbz79F7uTuHd09191zgWnA5fU8GACeAo41s0wzawoMIIxZ12efEnpKmNlBQFdgVaQVVUPs2MmDwAp3v2M3uyX9Mywteg5m9nfCWQvZZlYA/BJoAODu9wLPAiOAlcBmwjePOq0Kbb4JyALuiX2TLq3rs1lWoc31zp7a7O4rzGw28CawA3jA3Ss91be2q8Lf+VfAFDNbDhhhKLEuT+N9DHAusNzMlsW23Qh0gNR9hmn6DBERSaBhJRERSaBwEBGRBAoHERFJoHAQEZEECgcREUmgcBARkQQKBxERSfD/s3hJPbXaguUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koCp6yMYjgAm"
      },
      "source": [
        "# MODEL 2: SOIL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "adGKSG3ggTrr",
        "outputId": "ddb0c841-7ef6-4681-92ec-c3965b9941e8"
      },
      "source": [
        "print(\"net 2 out of 10: soil\")\n",
        "\n",
        "\n",
        "train_soil = os.path.join(tf_dir, 'trainingfile_soil.txt')\n",
        "val_soil = os.path.join(vf_dir, 'valfile_soil.txt')\n",
        "\n",
        "\n",
        "#train_soil = open(train_soil)\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_soil = AntarcticPlotDataset(train_soil, train_dir, transform=train_transform)\n",
        "val_data_soil = AntarcticPlotDataset(val_soil, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_soil = torch.utils.data.DataLoader(train_data_soil, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_soil = torch.utils.data.DataLoader(val_data_soil,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "soil_model = AntarcticNet()\n",
        "\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'soil_model.pt')\n",
        "torch.save(soil_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_soil)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net 2 out of 10: soil\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-ce52403aab4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# define datasets:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_data_soil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAntarcticPlotDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_soil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mval_data_soil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAntarcticPlotDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_soil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-dfd2701c885e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, txt_file, root_dir, transform)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;31m#print(np.asarray(img))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m#print(img.size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be 2/3 dimensional. Got {} dimensions.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 0 dimensions."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzNSM45WgyNV"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    soil_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_soil):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = soil_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_soil.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    soil_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_rock):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = soil_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(soil_model, PATH)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LyIjiZeh2UC"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uSNp-nejobT"
      },
      "source": [
        "# MODEL 3 - MORIBUND MOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kanICPkkYnn"
      },
      "source": [
        "print(\"net 3 of 11: moribund moss\")\n",
        "\n",
        "\n",
        "train_momo = os.path.join(tf_dir, 'trainingfile_momo.txt')\n",
        "val_momo = os.path.join(vf_dir, 'valfile_momo.txt')\n",
        "\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_momo = AntarcticPlotDataset(train_momo, train_dir, transform=train_transform)\n",
        "val_data_momo = AntarcticPlotDataset(val_momo, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_momo = torch.utils.data.DataLoader(train_data_momo, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_momo = torch.utils.data.DataLoader(val_data_momo,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# define model\n",
        "momo_model = models.resnet18(pretrained=False)\n",
        "momo_model.fc = nn.Linear(in_features=512, out_features=1)\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'momo_model.pt')\n",
        "torch.save(momo_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_momo)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxhbgH0bjwEE"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    momo_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_momo):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = momo_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_momo.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    momo_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_rock):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = momo_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(momo_model, PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cdQIE4lkJq5"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTMrnivYkNDS"
      },
      "source": [
        "# MODEL 4 - WHITE LICHEN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f03RuxmkU2l"
      },
      "source": [
        "train_WL = os.path.join(tf_dir, 'trainingfile_whli.txt')\n",
        "val_WL = os.path.join(vf_dir, 'valfile_whli.txt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_WL = AntarcticPlotDataset(train_WL, train_dir, transform=train_transform)\n",
        "val_data_WL = AntarcticPlotDataset(val_WL, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_WL = torch.utils.data.DataLoader(train_data_WL, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_WL = torch.utils.data.DataLoader(val_data_WL,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# define model\n",
        "WL_model = AntarcticNet()\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'wlichen_model.pt')\n",
        "torch.save(WL_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_WL)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itlaIQMVllzd"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    WL_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_WL):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = WL_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_WL.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    WL_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_rock):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = WL_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(WL_model, PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK43zOiymJ1U"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdBoJ7Oam8Lm"
      },
      "source": [
        "#Model 5 - Bryum Spo/Dead Moss/Other random unclassifiable things"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azEbWIcnBZS"
      },
      "source": [
        "train_bry = os.path.join(tf_dir, 'trainingfile_rand.txt')\n",
        "val_bry = os.path.join(vf_dir, 'valfile_rand.txt')\n",
        "\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_bry = AntarcticPlotDataset(train_bry, train_dir, transform=train_transform)\n",
        "val_data_bry = AntarcticPlotDataset(val_bry, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_bry = torch.utils.data.DataLoader(train_data_bry, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_bry = torch.utils.data.DataLoader(val_data_bry,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# define model\n",
        "bry_model = AntarcticNet\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'bryum_model.pt')\n",
        "torch.save(bry_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_bry)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUFpswpvnMuI"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    bry_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_bry):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = bry_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_bry.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    bry_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_rock):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = bry_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(bry_model, PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eZO8BeFn2sS"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54cj568aobuJ"
      },
      "source": [
        "#Model 6 - Sanionia Moss (brown moss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZU0xAB2ow5B"
      },
      "source": [
        "\n",
        "\n",
        "train_san = os.path.join(tf_dir, 'trainingfile_brmo.txt')\n",
        "val_san = os.path.join(vf_dir, 'valfile_brmo.txt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_san = AntarcticPlotDataset(train_san, train_dir, transform=train_transform)\n",
        "val_data_san = AntarcticPlotDataset(val_san, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_san = torch.utils.data.DataLoader(train_data_san, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_san = torch.utils.data.DataLoader(val_data_san,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# define model\n",
        "san_model = AntarcticNet()\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'sanionia_model.pt')\n",
        "torch.save(san_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_san)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgZQmHL6o-FJ"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    san_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_san):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = san_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_san.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    san_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_rock):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = san_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(san_model, PATH)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9on-JOopMIq"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmPRWR_qpQ59"
      },
      "source": [
        "#Model 7 - Hairgrass/grass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftrDCfIhpUtx"
      },
      "source": [
        "\n",
        "\n",
        "train_grass = os.path.join(tf_dir, 'trainingfile_gras.txt')\n",
        "val_grass = os.path.join(vf_dir, 'valfile_gras.txt')\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_grass = AntarcticPlotDataset(train_grass, train_dir, transform=train_transform)\n",
        "val_data_grass = AntarcticPlotDataset(val_grass, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_grass = torch.utils.data.DataLoader(train_data_grass, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_grass = torch.utils.data.DataLoader(val_data_grass,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# define model\n",
        "grass_model = AntarcticNet()\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'grass_model.pt')\n",
        "torch.save(grass_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_grass)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciyMZpcFpvOX"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    grass_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_grass):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = grass_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_grass.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    grass_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_rock):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = grass_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(grass_model, PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if2noX8ZqCLm"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj7S-NcDqZmd"
      },
      "source": [
        "#Model 8 - polytrichium strictum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kio786Wyqo3F"
      },
      "source": [
        "train_PS = os.path.join(tf_dir, 'trainingfile_pomo.txt')\n",
        "val_PS = os.path.join(vf_dir, 'valfile_pomo.txt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_PS = AntarcticPlotDataset(train_PS, train_dir, transform=train_transform)\n",
        "val_data_PS = AntarcticPlotDataset(val_PS, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_PS = torch.utils.data.DataLoader(train_data_PS, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_PS = torch.utils.data.DataLoader(val_data_PS,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# define model\n",
        "PS_model = AntarcticNet()\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'polytrich_model.pt')\n",
        "torch.save(PS_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_PS)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy44g6FWq6-C"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    PS_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_PS):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = PS_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_PS.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    PS_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_PS):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = PS_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(PS_model, PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIPnyDacrIK_"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6GaQRHnrNpR"
      },
      "source": [
        "#Model 9 - Chorisodontium Aciphyllum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b38XETMPrWLN"
      },
      "source": [
        "train_chor = os.path.join(tf_dir, 'trainingfile_chmo.txt')\n",
        "val_chor = os.path.join(vf_dir, 'valfile_chmo.txt')\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_chor = AntarcticPlotDataset(train_chor, train_dir, transform=train_transform)\n",
        "val_data_chor = AntarcticPlotDataset(val_chor, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_chor = torch.utils.data.DataLoader(train_data_chor, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_chor = torch.utils.data.DataLoader(val_data_chor,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# define model\n",
        "chor_model = AntarcticNet()\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'chor_model.pt')\n",
        "torch.save(chor_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_chor)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9mhFieCrd1k"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    chor_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_chor):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = chor_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_chor.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    chor_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_chor):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = chor_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(chor_model, PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4-4yUAzsDPP"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsJMCrW4sEEC"
      },
      "source": [
        "#Model 10 - Algae on rocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mzQWPFCsI7P"
      },
      "source": [
        "train_alg = os.path.join(tf_dir, 'trainingfile_alga.txt')\n",
        "val_alg = os.path.join(vf_dir, 'valfile_alga.txt')\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_alg = AntarcticPlotDataset(train_alg, train_dir, transform=train_transform)\n",
        "val_data_alg = AntarcticPlotDataset(val_alg, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_alg = torch.utils.data.DataLoader(train_data_alg, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_alg = torch.utils.data.DataLoader(val_data_alg,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# define model\n",
        "alg_model = AntarcticNet()\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'alg_model.pt')\n",
        "torch.save(alg_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_alg)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmOk0CYQsQzN"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    alg_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_alg):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = alg_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_alg.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    alg_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_alg):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = alg_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            algize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(alg_model, PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jryk7-uYsklt"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}