{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "texas tech google test",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJXrsPstyAEA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0EW0WB_pyaI"
      },
      "source": [
        "# INSTRUCTIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLzZsKoxM97S"
      },
      "source": [
        "Copy this notebook to your Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXmvX5uXajB"
      },
      "source": [
        "Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0S4wevQXY0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595776e9-41d6-4319-d506-d049ffa8207d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEsOmGaI9Fpk"
      },
      "source": [
        "# general path:\n",
        "data_path = \"/content/gdrive/My Drive/data/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV4K7gCDXsLs"
      },
      "source": [
        "Follow the directions from [here](https://mtl.yyliu.net/download/) to download the miniImageNet dataset. You should download 3 .tar files: train.tar, val.tar and test.tar.\n",
        "\n",
        "Put them in the ```/content/gdrive/My\\ Drive/data/miniImageNet/``` folder on your Google Drive. If you don't have such folder, create it first.\n",
        "\n",
        "Move to that folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOC25sFyXulw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e02d31f2-84bf-4976-86a2-c951174d4357"
      },
      "source": [
        "# go to the folder:\n",
        "%cd /content/gdrive/My\\ Drive/data\n",
        "# print out the content of the folder:\n",
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/data\n",
            "\u001b[0m\u001b[01;34mFlowers\u001b[0m/       \u001b[01;34mMNIST\u001b[0m/          \u001b[01;34mmodels\u001b[0m/         \u001b[01;34mtxtdata\u001b[0m/\n",
            "\u001b[01;34mminiImageNet\u001b[0m/  mnist_model.pt  \u001b[01;34mtrain_subsets\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88jTsSCI8ah8"
      },
      "source": [
        "Now untar the files. This might take few minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuNMweurjuod"
      },
      "source": [
        "Confirm correct creation of 3 folders (train, test and val):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5f37CvljrJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014cc0f2-b0d0-4f72-e499-5f3c4a16c405"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mFlowers\u001b[0m/       \u001b[01;34mMNIST\u001b[0m/          \u001b[01;34mmodels\u001b[0m/         \u001b[01;34mtxtdata\u001b[0m/\n",
            "\u001b[01;34mminiImageNet\u001b[0m/  mnist_model.pt  \u001b[01;34mtrain_subsets\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd5stBPKYDXE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7hWirg1yAEB"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqIfVxocyAED"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "train_dir = '/content/gdrive/My Drive/data/train_subsets'\n",
        "val_dir = '/content/gdrive/My Drive/data/train_subsets'\n",
        "model_dir = '/content/gdrive/My Drive/data/models'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z1NH-E--Ap5"
      },
      "source": [
        "Create a custom dataset where you will use your text files to store information about your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9OFI9KO-IZb"
      },
      "source": [
        "# data transforms:\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([transforms.Resize((224,224)), transforms.RandomHorizontalFlip(),\n",
        "                  transforms.RandomRotation(20), transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0),\n",
        "                  transforms.ToTensor(), normalize])\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), normalize]) \n",
        "\n",
        "\n",
        "# Create your custom dataset below:\n",
        "class AntarcticPlotDataset(Dataset):\n",
        "    \n",
        "    #initializing the master array\n",
        "    newdata = []\n",
        "    \n",
        "    def __init__(self, txt_file, root_dir, transform=None):\n",
        "        \n",
        "        \n",
        "        #directory where all the images are\n",
        "        self.root_dir = root_dir\n",
        "        \n",
        "        #transforms to do on the images\n",
        "        self.transform = transform\n",
        "        \n",
        "        #init\n",
        "        self.newdata = []\n",
        "        \n",
        "        \n",
        "        #counter for next() method, no longer used\n",
        "        self.counter = -1\n",
        "        \n",
        "        #text file w/ target data for the images\n",
        "        photoData = txt_file\n",
        "        \n",
        "        \n",
        "        #reading the text file line by line \n",
        "        \n",
        "        for line in open(photoData):\n",
        "            info = line\n",
        "\n",
        "    \n",
        "            infolist = info.split(\" \")\n",
        "            \n",
        "            #data from one line\n",
        "            finaldata = []\n",
        "            \n",
        "            #since all the sub images for one image rest in a master folder, need to get inside that folder\n",
        "            imgname = infolist[0]\n",
        "            \n",
        "            #seperating the .png from the image name\n",
        "            containingFolder = imgname.split(\"-\")[0]\n",
        "            \n",
        "            \n",
        "            #going into the folder named after the prent image\n",
        "            img_dir = os.path.join(root_dir, containingFolder)           \n",
        "            \n",
        "            #using the text file to get the name of the image\n",
        "            img = cv2.imread(os.path.join(img_dir, imgname))\n",
        "    \n",
        "            \n",
        "            #formatting and transforming the images\n",
        "            img = np.array(img)\n",
        "            trans = transforms.ToPILImage()\n",
        "            img = trans(img)\n",
        "            #print(np.asarray(img))\n",
        "            #print(img.size)\n",
        "            \n",
        "            \n",
        "            img = self.transform(img)\n",
        "\n",
        "            finaldata.append(img)\n",
        "            \n",
        "            \n",
        "            \n",
        "            #extracting the target data and adding it to the array for the line\n",
        "            finaldata.append(int(info.split(\":\")[1].strip('\\n')))\n",
        "            \n",
        "            \n",
        "            #adding the information from one line to the full master array\n",
        "            self.newdata.append(finaldata)\n",
        "\n",
        "\n",
        "            \n",
        "            \n",
        "                    \n",
        "    def __len__(self):\n",
        "        return len(self.newdata)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "         #gets the ith item in newdata\n",
        "        \n",
        "         #error check\n",
        "         if (i < 0 or i > len(self.newdata)):\n",
        "            print(\"problem\")\n",
        "            return None\n",
        "        \n",
        "         else:\n",
        "            \n",
        "            #extracting image      \n",
        "            img = self.newdata[i][0]\n",
        "            \n",
        "            #extracting target\n",
        "            landmarks = (self.newdata[i][1])\n",
        "            target = []\n",
        "            target.append(landmarks)\n",
        "            #storing both image in target in a dictionary format\n",
        "            sample = {'image' : img, 'landmarks' : landmarks}\n",
        "            #returning the dictionary - the enumerator wants a dictionary object\n",
        "            return sample\n",
        "\n",
        "\n",
        "\n",
        "train_rock = '/content/gdrive/My Drive/data/txtdata/trainingfiles/trainingfile_rock_small.txt'\n",
        "val_rock = '/content/gdrive/My Drive/data/txtdata/valfiles/valfile_rock.txt'\n",
        "\n",
        "train_data_rock = AntarcticPlotDataset(train_rock, train_dir, transform=train_transform)\n",
        "val_data_rock = AntarcticPlotDataset(val_rock, val_dir, transform=test_transform)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYh5DfWX_DdT"
      },
      "source": [
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0 # means to use all\n",
        "# how many samples per batch to load\n",
        "batch_size = 7\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tl_rock = torch.utils.data.DataLoader(train_data_rock, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_rock = torch.utils.data.DataLoader(val_data_rock,  num_workers = 0, batch_size=batch_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TighTVKsyAEJ"
      },
      "source": [
        "Define the network:\n",
        "\n",
        "We will use an existing network. Copy the ConvNet.py file from [here](https://drive.google.com/file/d/1Hf6j95u88qwJM3TnsWeiKGLTxaFHh4h4/view?usp=sharing) and put it in your \"miniImageNet\" folder on your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd547-kayAEJ"
      },
      "source": [
        "class AntarcticNet(nn.Module):\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        super(AntarcticNet, self).__init__()\n",
        "\n",
        "        #planning for 3+1 stacks \n",
        "        \n",
        "        #bias is default true\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 10, kernel_size = 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(10, 16, kernel_size = 3, stride=1, padding=1)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(16, 24, kernel_size = 3,stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(24, 32, kernel_size = 3, stride=1, padding=1)\n",
        "        \n",
        "        self.conv5 = nn.Conv2d(32, 44, kernel_size = 3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(44, 64, kernel_size = 3, stride=1, padding=1)\n",
        "        \n",
        "        #for stack 4\n",
        "        \n",
        "        self.conv7 = nn.Conv2d(64, 81, kernel_size = 3, stride=1, padding=1)\n",
        "        self.conv8 = nn.Conv2d(81, 96, kernel_size = 3, stride =1, padding=1)\n",
        "        \n",
        "        #for stack 5\n",
        "        \n",
        "        self.conv9 = nn.Conv2d(96, 128, kernel_size = 3, stride=1, padding=1)\n",
        "        self.conv10 = nn.Conv2d(128, 128, kernel_size = 3, stride =1, padding=1)\n",
        "        \n",
        "        \n",
        "        # lin conversion\n",
        "        \n",
        "        \n",
        "        self.lin = nn.Linear(in_features=6272, out_features=128)\n",
        "        self.lin2 = nn.Linear(in_features = 128, out_features=1)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(kernel_size = 3, stride=(2,2), padding=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #stack 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(self.pool(x))\n",
        "        \n",
        "        \n",
        "        #stack 2\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = torch.relu(self.pool(x))\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        #stack 3\n",
        "        \n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = torch.relu(self.pool(x))\n",
        "\n",
        "\n",
        "\n",
        "        #stack 4\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = torch.relu(self.pool(x))\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        #stack 5\n",
        "        \n",
        "        x = self.conv9(x)\n",
        "        x = torch.relu(self.pool(x)) #last pooling layer\n",
        "\n",
        "        \n",
        "        \n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        x = torch.relu(self.lin(x))\n",
        "        return self.lin2(x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhRDTUgGUfVK"
      },
      "source": [
        "Now let's specify the loss function that we will use.\n",
        "For this reason let's adapt Contrastive Loss implemented [here](https://github.com/adambielski/siamese-triplet/blob/master/losses.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pMld9zBfEz3"
      },
      "source": [
        "Let's create a function that will take our model as a parameter and will choose n images from each category and generate their embeddings (extract feature vectors that describe the images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT18rNBA96i-"
      },
      "source": [
        "Let's read the information about the validation and test splits and store it in two lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQy7FBy-CNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef043119-320d-46fa-e3b5-c0425ad5aaa0"
      },
      "source": [
        "rock_model = AntarcticNet()\n",
        "\n",
        "\n",
        "# specify loss function\n",
        "#criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = nn.MSELoss()\n",
        "#criterion = nn.BCELoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.SGD(rock_model.parameters(), lr= 0.01, momentum=0.9)\n",
        "\n",
        "# specify scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'rock_model.pt')\n",
        "torch.save(rock_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_epochs = 20\n",
        "n_iterations = int(len(train_data_rock)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []\n",
        "\n",
        "\n",
        "rock_model.train()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AntarcticNet(\n",
              "  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5): Conv2d(32, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv6): Conv2d(44, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv7): Conv2d(64, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv8): Conv2d(81, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv9): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (lin): Linear(in_features=6272, out_features=128, bias=True)\n",
              "  (lin2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (pool): MaxPool2d(kernel_size=3, stride=(2, 2), padding=1, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXpAwuHy-De8"
      },
      "source": [
        "Extract embeddings:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOg4ig-hfpuu"
      },
      "source": [
        "Let's create a testing routine to predict samples in the val/test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBpytd08yAEh"
      },
      "source": [
        "Training the network.\n",
        "\n",
        "We will iterate through our dataset. For evey iteration we need to:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhWP7P7CyAEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044aeba9-173e-42f2-826f-ca144bc12c75"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    rock_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_rock):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = rock_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_rock.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    rock_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_rock):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = rock_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(rock_model, PATH)\n",
        "\n",
        "traintext.close()\n",
        "valtext.close()\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0409],\n",
            "        [-0.0416],\n",
            "        [-0.0410],\n",
            "        [-0.0410],\n",
            "        [-0.0412],\n",
            "        [-0.0410],\n",
            "        [-0.0406]], grad_fn=<AddmmBackward>)\n",
            "tensor([[-0.0348],\n",
            "        [-0.0350],\n",
            "        [-0.0352],\n",
            "        [-0.0352],\n",
            "        [-0.0353],\n",
            "        [-0.0347],\n",
            "        [-0.0352]], grad_fn=<AddmmBackward>)\n",
            "tensor([[-0.0106],\n",
            "        [-0.0107],\n",
            "        [-0.0105],\n",
            "        [-0.0106],\n",
            "        [-0.0107],\n",
            "        [-0.0107],\n",
            "        [-0.0105]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.0200],\n",
            "        [0.0201],\n",
            "        [0.0201],\n",
            "        [0.0198],\n",
            "        [0.0199],\n",
            "        [0.0201],\n",
            "        [0.0204]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.0633],\n",
            "        [0.0635],\n",
            "        [0.0638],\n",
            "        [0.0633],\n",
            "        [0.0632],\n",
            "        [0.0629],\n",
            "        [0.0637]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1026],\n",
            "        [0.1021],\n",
            "        [0.1021],\n",
            "        [0.1024],\n",
            "        [0.1019],\n",
            "        [0.1021],\n",
            "        [0.1018]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1492],\n",
            "        [0.1511],\n",
            "        [0.1501],\n",
            "        [0.1495],\n",
            "        [0.1490],\n",
            "        [0.1494],\n",
            "        [0.1494]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 1 \tTraining Loss: 0.360753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the validation set: 41 %\n",
            "tensor([[0.1992],\n",
            "        [0.2012],\n",
            "        [0.1987],\n",
            "        [0.1994],\n",
            "        [0.1998],\n",
            "        [0.1997],\n",
            "        [0.1993]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2440],\n",
            "        [0.2432],\n",
            "        [0.2436],\n",
            "        [0.2448],\n",
            "        [0.2444],\n",
            "        [0.2440],\n",
            "        [0.2434]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2942],\n",
            "        [0.2943],\n",
            "        [0.2943],\n",
            "        [0.2971],\n",
            "        [0.2946],\n",
            "        [0.2949],\n",
            "        [0.2961]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3416],\n",
            "        [0.3404],\n",
            "        [0.3423],\n",
            "        [0.3400],\n",
            "        [0.3407],\n",
            "        [0.3425],\n",
            "        [0.3461]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3936],\n",
            "        [0.3930],\n",
            "        [0.3947],\n",
            "        [0.3924],\n",
            "        [0.3918],\n",
            "        [0.3905],\n",
            "        [0.3941]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4318],\n",
            "        [0.4296],\n",
            "        [0.4301],\n",
            "        [0.4313],\n",
            "        [0.4286],\n",
            "        [0.4296],\n",
            "        [0.4285]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4695],\n",
            "        [0.4761],\n",
            "        [0.4723],\n",
            "        [0.4707],\n",
            "        [0.4690],\n",
            "        [0.4701],\n",
            "        [0.4700]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 2 \tTraining Loss: 0.238773\n",
            "Accuracy of the network on the validation set: 58 %\n",
            "tensor([[0.5027],\n",
            "        [0.5084],\n",
            "        [0.5015],\n",
            "        [0.5035],\n",
            "        [0.5045],\n",
            "        [0.5043],\n",
            "        [0.5042]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5200],\n",
            "        [0.5167],\n",
            "        [0.5180],\n",
            "        [0.5202],\n",
            "        [0.5193],\n",
            "        [0.5188],\n",
            "        [0.5171]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5319],\n",
            "        [0.5322],\n",
            "        [0.5321],\n",
            "        [0.5391],\n",
            "        [0.5327],\n",
            "        [0.5331],\n",
            "        [0.5374]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5358],\n",
            "        [0.5339],\n",
            "        [0.5367],\n",
            "        [0.5333],\n",
            "        [0.5346],\n",
            "        [0.5382],\n",
            "        [0.5438]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5406],\n",
            "        [0.5396],\n",
            "        [0.5414],\n",
            "        [0.5385],\n",
            "        [0.5374],\n",
            "        [0.5356],\n",
            "        [0.5412]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5279],\n",
            "        [0.5238],\n",
            "        [0.5238],\n",
            "        [0.5261],\n",
            "        [0.5221],\n",
            "        [0.5231],\n",
            "        [0.5220]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5127],\n",
            "        [0.5211],\n",
            "        [0.5166],\n",
            "        [0.5134],\n",
            "        [0.5117],\n",
            "        [0.5130],\n",
            "        [0.5146]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 3 \tTraining Loss: 0.254588\n",
            "Accuracy of the network on the validation set: 94 %\n",
            "tensor([[0.4985],\n",
            "        [0.5036],\n",
            "        [0.4972],\n",
            "        [0.4997],\n",
            "        [0.4998],\n",
            "        [0.4999],\n",
            "        [0.5030]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4789],\n",
            "        [0.4735],\n",
            "        [0.4756],\n",
            "        [0.4762],\n",
            "        [0.4755],\n",
            "        [0.4763],\n",
            "        [0.4734]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4554],\n",
            "        [0.4555],\n",
            "        [0.4555],\n",
            "        [0.4626],\n",
            "        [0.4560],\n",
            "        [0.4559],\n",
            "        [0.4620]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4358],\n",
            "        [0.4349],\n",
            "        [0.4361],\n",
            "        [0.4344],\n",
            "        [0.4355],\n",
            "        [0.4388],\n",
            "        [0.4423]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4231],\n",
            "        [0.4222],\n",
            "        [0.4223],\n",
            "        [0.4214],\n",
            "        [0.4205],\n",
            "        [0.4195],\n",
            "        [0.4235]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4047],\n",
            "        [0.4008],\n",
            "        [0.4001],\n",
            "        [0.4026],\n",
            "        [0.3994],\n",
            "        [0.3997],\n",
            "        [0.3994]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3871],\n",
            "        [0.3934],\n",
            "        [0.3904],\n",
            "        [0.3871],\n",
            "        [0.3862],\n",
            "        [0.3871],\n",
            "        [0.3897]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 4 \tTraining Loss: 0.244498\n",
            "Accuracy of the network on the validation set: 41 %\n",
            "tensor([[0.3762],\n",
            "        [0.3791],\n",
            "        [0.3753],\n",
            "        [0.3771],\n",
            "        [0.3766],\n",
            "        [0.3768],\n",
            "        [0.3812]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3658],\n",
            "        [0.3604],\n",
            "        [0.3623],\n",
            "        [0.3620],\n",
            "        [0.3615],\n",
            "        [0.3630],\n",
            "        [0.3601]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3521],\n",
            "        [0.3521],\n",
            "        [0.3522],\n",
            "        [0.3582],\n",
            "        [0.3524],\n",
            "        [0.3522],\n",
            "        [0.3583]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3445],\n",
            "        [0.3442],\n",
            "        [0.3447],\n",
            "        [0.3438],\n",
            "        [0.3447],\n",
            "        [0.3477],\n",
            "        [0.3500]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3444],\n",
            "        [0.3433],\n",
            "        [0.3428],\n",
            "        [0.3428],\n",
            "        [0.3420],\n",
            "        [0.3413],\n",
            "        [0.3444]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3405],\n",
            "        [0.3363],\n",
            "        [0.3355],\n",
            "        [0.3380],\n",
            "        [0.3351],\n",
            "        [0.3351],\n",
            "        [0.3350]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3359],\n",
            "        [0.3420],\n",
            "        [0.3393],\n",
            "        [0.3356],\n",
            "        [0.3350],\n",
            "        [0.3358],\n",
            "        [0.3390]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 5 \tTraining Loss: 0.239523\n",
            "Accuracy of the network on the validation set: 41 %\n",
            "tensor([[0.3380],\n",
            "        [0.3407],\n",
            "        [0.3371],\n",
            "        [0.3389],\n",
            "        [0.3382],\n",
            "        [0.3385],\n",
            "        [0.3441]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3417],\n",
            "        [0.3350],\n",
            "        [0.3373],\n",
            "        [0.3367],\n",
            "        [0.3361],\n",
            "        [0.3379],\n",
            "        [0.3346]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3383],\n",
            "        [0.3382],\n",
            "        [0.3384],\n",
            "        [0.3457],\n",
            "        [0.3386],\n",
            "        [0.3383],\n",
            "        [0.3461]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3415],\n",
            "        [0.3413],\n",
            "        [0.3418],\n",
            "        [0.3408],\n",
            "        [0.3419],\n",
            "        [0.3459],\n",
            "        [0.3482]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3517],\n",
            "        [0.3502],\n",
            "        [0.3493],\n",
            "        [0.3494],\n",
            "        [0.3483],\n",
            "        [0.3475],\n",
            "        [0.3516]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3575],\n",
            "        [0.3516],\n",
            "        [0.3502],\n",
            "        [0.3539],\n",
            "        [0.3497],\n",
            "        [0.3498],\n",
            "        [0.3496]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3580],\n",
            "        [0.3670],\n",
            "        [0.3631],\n",
            "        [0.3575],\n",
            "        [0.3566],\n",
            "        [0.3577],\n",
            "        [0.3628]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 6 \tTraining Loss: 0.237107\n",
            "Accuracy of the network on the validation set: 41 %\n",
            "tensor([[0.3658],\n",
            "        [0.3700],\n",
            "        [0.3644],\n",
            "        [0.3674],\n",
            "        [0.3662],\n",
            "        [0.3667],\n",
            "        [0.3755]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3777],\n",
            "        [0.3672],\n",
            "        [0.3711],\n",
            "        [0.3698],\n",
            "        [0.3687],\n",
            "        [0.3718],\n",
            "        [0.3665]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3735],\n",
            "        [0.3732],\n",
            "        [0.3735],\n",
            "        [0.3855],\n",
            "        [0.3739],\n",
            "        [0.3734],\n",
            "        [0.3860]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3797],\n",
            "        [0.3793],\n",
            "        [0.3802],\n",
            "        [0.3786],\n",
            "        [0.3805],\n",
            "        [0.3869],\n",
            "        [0.3899]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3925],\n",
            "        [0.3903],\n",
            "        [0.3886],\n",
            "        [0.3888],\n",
            "        [0.3871],\n",
            "        [0.3855],\n",
            "        [0.3926]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4005],\n",
            "        [0.3911],\n",
            "        [0.3888],\n",
            "        [0.3946],\n",
            "        [0.3877],\n",
            "        [0.3878],\n",
            "        [0.3877]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3957],\n",
            "        [0.4104],\n",
            "        [0.4041],\n",
            "        [0.3947],\n",
            "        [0.3932],\n",
            "        [0.3951],\n",
            "        [0.4036]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 7 \tTraining Loss: 0.233784\n",
            "Accuracy of the network on the validation set: 41 %\n",
            "tensor([[0.4010],\n",
            "        [0.4084],\n",
            "        [0.3986],\n",
            "        [0.4038],\n",
            "        [0.4019],\n",
            "        [0.4027],\n",
            "        [0.4171]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4164],\n",
            "        [0.3991],\n",
            "        [0.4057],\n",
            "        [0.4034],\n",
            "        [0.4015],\n",
            "        [0.4065],\n",
            "        [0.3977]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4013],\n",
            "        [0.4006],\n",
            "        [0.4011],\n",
            "        [0.4215],\n",
            "        [0.4019],\n",
            "        [0.4011],\n",
            "        [0.4222]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4049],\n",
            "        [0.4042],\n",
            "        [0.4057],\n",
            "        [0.4028],\n",
            "        [0.4063],\n",
            "        [0.4172],\n",
            "        [0.4213]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4164],\n",
            "        [0.4127],\n",
            "        [0.4098],\n",
            "        [0.4097],\n",
            "        [0.4068],\n",
            "        [0.4040],\n",
            "        [0.4166]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4243],\n",
            "        [0.4080],\n",
            "        [0.4040],\n",
            "        [0.4141],\n",
            "        [0.4019],\n",
            "        [0.4023],\n",
            "        [0.4022]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4068],\n",
            "        [0.4327],\n",
            "        [0.4220],\n",
            "        [0.4051],\n",
            "        [0.4025],\n",
            "        [0.4056],\n",
            "        [0.4210]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 8 \tTraining Loss: 0.232618\n",
            "Accuracy of the network on the validation set: 41 %\n",
            "tensor([[0.4063],\n",
            "        [0.4204],\n",
            "        [0.4028],\n",
            "        [0.4118],\n",
            "        [0.4084],\n",
            "        [0.4099],\n",
            "        [0.4361]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4315],\n",
            "        [0.3996],\n",
            "        [0.4118],\n",
            "        [0.4073],\n",
            "        [0.4039],\n",
            "        [0.4128],\n",
            "        [0.3975]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3972],\n",
            "        [0.3960],\n",
            "        [0.3965],\n",
            "        [0.4343],\n",
            "        [0.3976],\n",
            "        [0.3966],\n",
            "        [0.4357]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3992],\n",
            "        [0.3985],\n",
            "        [0.4008],\n",
            "        [0.3953],\n",
            "        [0.4020],\n",
            "        [0.4234],\n",
            "        [0.4293]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4150],\n",
            "        [0.4076],\n",
            "        [0.4018],\n",
            "        [0.4013],\n",
            "        [0.3959],\n",
            "        [0.3920],\n",
            "        [0.4153]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4318],\n",
            "        [0.3986],\n",
            "        [0.3908],\n",
            "        [0.4106],\n",
            "        [0.3878],\n",
            "        [0.3890],\n",
            "        [0.3878]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3927],\n",
            "        [0.4456],\n",
            "        [0.4245],\n",
            "        [0.3899],\n",
            "        [0.3858],\n",
            "        [0.3902],\n",
            "        [0.4229]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 9 \tTraining Loss: 0.228462\n",
            "Accuracy of the network on the validation set: 41 %\n",
            "tensor([[0.3884],\n",
            "        [0.4193],\n",
            "        [0.3840],\n",
            "        [0.3999],\n",
            "        [0.3941],\n",
            "        [0.3967],\n",
            "        [0.4520]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4489],\n",
            "        [0.3800],\n",
            "        [0.4064],\n",
            "        [0.3965],\n",
            "        [0.3896],\n",
            "        [0.4076],\n",
            "        [0.3773]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3773],\n",
            "        [0.3747],\n",
            "        [0.3752],\n",
            "        [0.4581],\n",
            "        [0.3768],\n",
            "        [0.3753],\n",
            "        [0.4608]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3844],\n",
            "        [0.3845],\n",
            "        [0.3880],\n",
            "        [0.3751],\n",
            "        [0.3898],\n",
            "        [0.4436],\n",
            "        [0.4541]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4261],\n",
            "        [0.4070],\n",
            "        [0.3927],\n",
            "        [0.3889],\n",
            "        [0.3755],\n",
            "        [0.3694],\n",
            "        [0.4279]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4806],\n",
            "        [0.3940],\n",
            "        [0.3739],\n",
            "        [0.4252],\n",
            "        [0.3678],\n",
            "        [0.3708],\n",
            "        [0.3668]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3814],\n",
            "        [0.5289],\n",
            "        [0.4717],\n",
            "        [0.3741],\n",
            "        [0.3643],\n",
            "        [0.3740],\n",
            "        [0.4659]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 10 \tTraining Loss: 0.214425\n",
            "Accuracy of the network on the validation set: 70 %\n",
            "tensor([[0.3689],\n",
            "        [0.4708],\n",
            "        [0.3595],\n",
            "        [0.4070],\n",
            "        [0.3911],\n",
            "        [0.3974],\n",
            "        [0.5588]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5723],\n",
            "        [0.3573],\n",
            "        [0.4418],\n",
            "        [0.4145],\n",
            "        [0.3924],\n",
            "        [0.4421],\n",
            "        [0.3514]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3554],\n",
            "        [0.3464],\n",
            "        [0.3467],\n",
            "        [0.6278],\n",
            "        [0.3509],\n",
            "        [0.3476],\n",
            "        [0.6402]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3901],\n",
            "        [0.3924],\n",
            "        [0.3999],\n",
            "        [0.3494],\n",
            "        [0.3955],\n",
            "        [0.6092],\n",
            "        [0.6625]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5830],\n",
            "        [0.4993],\n",
            "        [0.4400],\n",
            "        [0.4012],\n",
            "        [0.3420],\n",
            "        [0.3328],\n",
            "        [0.5923]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.8332],\n",
            "        [0.4329],\n",
            "        [0.3494],\n",
            "        [0.5732],\n",
            "        [0.3413],\n",
            "        [0.3536],\n",
            "        [0.3263]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3581],\n",
            "        [1.1339],\n",
            "        [0.8090],\n",
            "        [0.3536],\n",
            "        [0.3183],\n",
            "        [0.3318],\n",
            "        [0.7544]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 11 \tTraining Loss: 0.167977\n",
            "Accuracy of the network on the validation set: 93 %\n",
            "tensor([[0.3080],\n",
            "        [0.8282],\n",
            "        [0.2964],\n",
            "        [0.4689],\n",
            "        [0.4218],\n",
            "        [0.4392],\n",
            "        [1.2641]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0614],\n",
            "        [0.2731],\n",
            "        [0.5119],\n",
            "        [0.4298],\n",
            "        [0.3635],\n",
            "        [0.5136],\n",
            "        [0.2705]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2511],\n",
            "        [0.2500],\n",
            "        [0.2501],\n",
            "        [1.0292],\n",
            "        [0.2504],\n",
            "        [0.2496],\n",
            "        [1.0816]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2424],\n",
            "        [0.2727],\n",
            "        [0.2488],\n",
            "        [0.2277],\n",
            "        [0.2302],\n",
            "        [0.7371],\n",
            "        [0.8033]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4176],\n",
            "        [0.2606],\n",
            "        [0.2310],\n",
            "        [0.2174],\n",
            "        [0.2127],\n",
            "        [0.2121],\n",
            "        [0.4339]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.8713],\n",
            "        [0.2030],\n",
            "        [0.1954],\n",
            "        [0.4138],\n",
            "        [0.2020],\n",
            "        [0.1975],\n",
            "        [0.1967]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1890],\n",
            "        [1.0773],\n",
            "        [0.6968],\n",
            "        [0.1883],\n",
            "        [0.1874],\n",
            "        [0.1881],\n",
            "        [0.6956]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 12 \tTraining Loss: 0.147549\n",
            "Accuracy of the network on the validation set: 90 %\n",
            "tensor([[0.1816],\n",
            "        [0.5483],\n",
            "        [0.1796],\n",
            "        [0.2391],\n",
            "        [0.2083],\n",
            "        [0.1988],\n",
            "        [1.2474]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.1321],\n",
            "        [0.1665],\n",
            "        [0.4406],\n",
            "        [0.2387],\n",
            "        [0.1908],\n",
            "        [0.4677],\n",
            "        [0.1642]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1567],\n",
            "        [0.1560],\n",
            "        [0.1565],\n",
            "        [1.2117],\n",
            "        [0.1577],\n",
            "        [0.1555],\n",
            "        [1.2710]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1687],\n",
            "        [0.2104],\n",
            "        [0.1900],\n",
            "        [0.1488],\n",
            "        [0.1733],\n",
            "        [0.8781],\n",
            "        [0.8998]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4948],\n",
            "        [0.3117],\n",
            "        [0.1874],\n",
            "        [0.1662],\n",
            "        [0.1435],\n",
            "        [0.1412],\n",
            "        [0.6129]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.9819],\n",
            "        [0.3249],\n",
            "        [0.1351],\n",
            "        [0.5863],\n",
            "        [0.1495],\n",
            "        [0.1408],\n",
            "        [0.1359]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2529],\n",
            "        [1.1770],\n",
            "        [0.8798],\n",
            "        [0.1530],\n",
            "        [0.1332],\n",
            "        [0.1613],\n",
            "        [0.8339]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 13 \tTraining Loss: 0.118103\n",
            "Accuracy of the network on the validation set: 92 %\n",
            "tensor([[0.1511],\n",
            "        [0.6875],\n",
            "        [0.1294],\n",
            "        [0.4224],\n",
            "        [0.2495],\n",
            "        [0.2975],\n",
            "        [1.1713]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.9422],\n",
            "        [0.1291],\n",
            "        [0.6304],\n",
            "        [0.2884],\n",
            "        [0.1908],\n",
            "        [0.6504],\n",
            "        [0.1161]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1137],\n",
            "        [0.1085],\n",
            "        [0.1091],\n",
            "        [0.9339],\n",
            "        [0.1113],\n",
            "        [0.1083],\n",
            "        [0.9387]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1867],\n",
            "        [0.2099],\n",
            "        [0.2738],\n",
            "        [0.1162],\n",
            "        [0.3921],\n",
            "        [0.7937],\n",
            "        [0.7317]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5814],\n",
            "        [0.5130],\n",
            "        [0.2633],\n",
            "        [0.3423],\n",
            "        [0.1357],\n",
            "        [0.1017],\n",
            "        [0.7331]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.9083],\n",
            "        [0.4635],\n",
            "        [0.1061],\n",
            "        [0.6482],\n",
            "        [0.1097],\n",
            "        [0.0976],\n",
            "        [0.1004]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2877],\n",
            "        [1.0534],\n",
            "        [0.8755],\n",
            "        [0.0982],\n",
            "        [0.0888],\n",
            "        [0.1474],\n",
            "        [0.8556]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 14 \tTraining Loss: 0.105677\n",
            "Accuracy of the network on the validation set: 91 %\n",
            "tensor([[0.0916],\n",
            "        [0.5168],\n",
            "        [0.0834],\n",
            "        [0.3200],\n",
            "        [0.1246],\n",
            "        [0.1697],\n",
            "        [1.1208]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0459],\n",
            "        [0.0765],\n",
            "        [0.7308],\n",
            "        [0.1592],\n",
            "        [0.0815],\n",
            "        [0.7967],\n",
            "        [0.0694]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.0642],\n",
            "        [0.0629],\n",
            "        [0.0640],\n",
            "        [1.1256],\n",
            "        [0.0677],\n",
            "        [0.0624],\n",
            "        [1.1222]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.0849],\n",
            "        [0.1305],\n",
            "        [0.1622],\n",
            "        [0.0671],\n",
            "        [0.2866],\n",
            "        [0.9750],\n",
            "        [0.7393]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4499],\n",
            "        [0.2858],\n",
            "        [0.1008],\n",
            "        [0.1397],\n",
            "        [0.0702],\n",
            "        [0.0638],\n",
            "        [0.7674]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.1797],\n",
            "        [0.5184],\n",
            "        [0.0672],\n",
            "        [0.8281],\n",
            "        [0.0839],\n",
            "        [0.0638],\n",
            "        [0.0696]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3279],\n",
            "        [1.3119],\n",
            "        [1.1475],\n",
            "        [0.0769],\n",
            "        [0.0681],\n",
            "        [0.1291],\n",
            "        [1.1484]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 15 \tTraining Loss: 0.091351\n",
            "Accuracy of the network on the validation set: 91 %\n",
            "tensor([[0.0788],\n",
            "        [0.5256],\n",
            "        [0.0657],\n",
            "        [0.3538],\n",
            "        [0.1099],\n",
            "        [0.1596],\n",
            "        [1.2170]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.1802],\n",
            "        [0.0857],\n",
            "        [0.9683],\n",
            "        [0.2563],\n",
            "        [0.1109],\n",
            "        [1.0204],\n",
            "        [0.0653]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.0673],\n",
            "        [0.0634],\n",
            "        [0.0646],\n",
            "        [1.2293],\n",
            "        [0.0680],\n",
            "        [0.0635],\n",
            "        [1.1906]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1787],\n",
            "        [0.2152],\n",
            "        [0.3522],\n",
            "        [0.0906],\n",
            "        [0.6061],\n",
            "        [1.0962],\n",
            "        [0.8745]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.5038],\n",
            "        [0.3909],\n",
            "        [0.1235],\n",
            "        [0.2340],\n",
            "        [0.0720],\n",
            "        [0.0639],\n",
            "        [0.8329]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0809],\n",
            "        [0.5796],\n",
            "        [0.0758],\n",
            "        [0.8232],\n",
            "        [0.0901],\n",
            "        [0.0670],\n",
            "        [0.0763]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2700],\n",
            "        [1.0699],\n",
            "        [0.9801],\n",
            "        [0.0673],\n",
            "        [0.0620],\n",
            "        [0.0939],\n",
            "        [0.9954]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 16 \tTraining Loss: 0.073476\n",
            "Accuracy of the network on the validation set: 91 %\n",
            "tensor([[0.0705],\n",
            "        [0.4321],\n",
            "        [0.0605],\n",
            "        [0.2949],\n",
            "        [0.0854],\n",
            "        [0.1242],\n",
            "        [1.0776]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0499],\n",
            "        [0.0735],\n",
            "        [0.8777],\n",
            "        [0.2003],\n",
            "        [0.0838],\n",
            "        [0.9340],\n",
            "        [0.0599]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.0603],\n",
            "        [0.0579],\n",
            "        [0.0592],\n",
            "        [1.0971],\n",
            "        [0.0627],\n",
            "        [0.0581],\n",
            "        [1.0597]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1459],\n",
            "        [0.1849],\n",
            "        [0.3019],\n",
            "        [0.0781],\n",
            "        [0.5411],\n",
            "        [0.9977],\n",
            "        [0.7631]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4262],\n",
            "        [0.3235],\n",
            "        [0.1029],\n",
            "        [0.1934],\n",
            "        [0.0669],\n",
            "        [0.0590],\n",
            "        [0.7515]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0011],\n",
            "        [0.5301],\n",
            "        [0.0677],\n",
            "        [0.7621],\n",
            "        [0.0794],\n",
            "        [0.0607],\n",
            "        [0.0694]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2468],\n",
            "        [0.9871],\n",
            "        [0.9208],\n",
            "        [0.0627],\n",
            "        [0.0580],\n",
            "        [0.0810],\n",
            "        [0.9408]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 17 \tTraining Loss: 0.071122\n",
            "Accuracy of the network on the validation set: 90 %\n",
            "tensor([[0.0669],\n",
            "        [0.3978],\n",
            "        [0.0572],\n",
            "        [0.2751],\n",
            "        [0.0770],\n",
            "        [0.1116],\n",
            "        [1.0180]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0094],\n",
            "        [0.0713],\n",
            "        [0.8582],\n",
            "        [0.1890],\n",
            "        [0.0771],\n",
            "        [0.9139],\n",
            "        [0.0573]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.0585],\n",
            "        [0.0560],\n",
            "        [0.0573],\n",
            "        [1.0717],\n",
            "        [0.0608],\n",
            "        [0.0562],\n",
            "        [1.0319]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1495],\n",
            "        [0.1859],\n",
            "        [0.3118],\n",
            "        [0.0796],\n",
            "        [0.5614],\n",
            "        [0.9933],\n",
            "        [0.7569]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4410],\n",
            "        [0.3487],\n",
            "        [0.1062],\n",
            "        [0.2134],\n",
            "        [0.0668],\n",
            "        [0.0581],\n",
            "        [0.7688]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0151],\n",
            "        [0.5637],\n",
            "        [0.0709],\n",
            "        [0.7870],\n",
            "        [0.0816],\n",
            "        [0.0612],\n",
            "        [0.0715]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2810],\n",
            "        [1.0130],\n",
            "        [0.9507],\n",
            "        [0.0637],\n",
            "        [0.0580],\n",
            "        [0.1003],\n",
            "        [0.9666]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 18 \tTraining Loss: 0.069798\n",
            "Accuracy of the network on the validation set: 91 %\n",
            "tensor([[0.0684],\n",
            "        [0.4257],\n",
            "        [0.0572],\n",
            "        [0.3020],\n",
            "        [0.0826],\n",
            "        [0.1258],\n",
            "        [1.0436]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0408],\n",
            "        [0.0772],\n",
            "        [0.8991],\n",
            "        [0.2124],\n",
            "        [0.0849],\n",
            "        [0.9507],\n",
            "        [0.0576]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.0596],\n",
            "        [0.0561],\n",
            "        [0.0574],\n",
            "        [1.1125],\n",
            "        [0.0611],\n",
            "        [0.0564],\n",
            "        [1.0675]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1673],\n",
            "        [0.1992],\n",
            "        [0.3455],\n",
            "        [0.0872],\n",
            "        [0.6138],\n",
            "        [1.0340],\n",
            "        [0.7966]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4796],\n",
            "        [0.3944],\n",
            "        [0.1151],\n",
            "        [0.2459],\n",
            "        [0.0688],\n",
            "        [0.0580],\n",
            "        [0.8151]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0528],\n",
            "        [0.6059],\n",
            "        [0.0762],\n",
            "        [0.8278],\n",
            "        [0.0849],\n",
            "        [0.0624],\n",
            "        [0.0744]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.3096],\n",
            "        [1.0512],\n",
            "        [0.9893],\n",
            "        [0.0643],\n",
            "        [0.0574],\n",
            "        [0.1178],\n",
            "        [1.0028]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 19 \tTraining Loss: 0.068667\n",
            "Accuracy of the network on the validation set: 91 %\n",
            "tensor([[0.0686],\n",
            "        [0.4459],\n",
            "        [0.0561],\n",
            "        [0.3184],\n",
            "        [0.0849],\n",
            "        [0.1326],\n",
            "        [1.0731]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0663],\n",
            "        [0.0780],\n",
            "        [0.9272],\n",
            "        [0.2204],\n",
            "        [0.0853],\n",
            "        [0.9789],\n",
            "        [0.0561]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.0577],\n",
            "        [0.0542],\n",
            "        [0.0555],\n",
            "        [1.1362],\n",
            "        [0.0592],\n",
            "        [0.0545],\n",
            "        [1.0879]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.1645],\n",
            "        [0.1974],\n",
            "        [0.3475],\n",
            "        [0.0843],\n",
            "        [0.6218],\n",
            "        [1.0513],\n",
            "        [0.8044]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.4711],\n",
            "        [0.3833],\n",
            "        [0.1098],\n",
            "        [0.2362],\n",
            "        [0.0657],\n",
            "        [0.0553],\n",
            "        [0.8198]], grad_fn=<AddmmBackward>)\n",
            "tensor([[1.0596],\n",
            "        [0.6003],\n",
            "        [0.0710],\n",
            "        [0.8304],\n",
            "        [0.0804],\n",
            "        [0.0585],\n",
            "        [0.0704]], grad_fn=<AddmmBackward>)\n",
            "tensor([[0.2902],\n",
            "        [1.0458],\n",
            "        [0.9891],\n",
            "        [0.0601],\n",
            "        [0.0540],\n",
            "        [0.0998],\n",
            "        [1.0068]], grad_fn=<AddmmBackward>)\n",
            "Epoch: 20 \tTraining Loss: 0.068302\n",
            "Accuracy of the network on the validation set: 91 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2kZs54yGNeY"
      },
      "source": [
        "After training, test the model on the test set. Write the code for it below. Use the example of validation set as your reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_4KVbb-GePi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "495848c3-17af-4d35-8e54-a32cb8c41b18"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adGKSG3ggTrr"
      },
      "source": [
        "print(\"net 2 out of 10: soil\")\n",
        "\n",
        "\n",
        "train_soil = os.path.join(tf_dir, 'trainingfile_soil.txt')\n",
        "val_soil = os.path.join(vf_dir, 'valfile_soil.txt')\n",
        "\n",
        "traintext = open(train_soil, \"r\")\n",
        "valtext = open(val_soil, \"r\")\n",
        "\n",
        "\n",
        "# define datasets:\n",
        "\n",
        "train_data_soil = AntarcticPlotDataset(traintext, train_dir, transform=train_transform)\n",
        "val_data_soil = AntarcticPlotDataset(valtext, val_dir, transform=test_transform)\n",
        "\n",
        "\n",
        "#test_data = datasets.ImageFolder(\"./output_data_rocks\", transform=test_transform)\n",
        "\n",
        "# load the data in batches: \n",
        "\n",
        "tl_soil = torch.utils.data.DataLoader(train_data_soil, num_workers = 0, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "vl_soil = torch.utils.data.DataLoader(val_data_soil,  num_workers = 0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "soil_model = AntarcticNet()\n",
        "\n",
        "\n",
        "# save model\n",
        "PATH = os.path.join(model_dir, 'soil_model.pt')\n",
        "torch.save(soil_model, PATH)\n",
        "\n",
        "\n",
        "# number of epochs to train the model, number of iterations per epoch\n",
        "n_iterations = int(len(train_data_soil)/batch_size)\n",
        "\n",
        "# lists to keep track of training progress:\n",
        "train_loss_progress = []\n",
        "validation_accuracy_progress = []\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzNSM45WgyNV"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    #load the model\n",
        "    \n",
        "    #prep to train\n",
        "    soil_model.train()\n",
        "        \n",
        "    \n",
        "    \n",
        "    for iter, D in enumerate(tl_soil):  \n",
        "        \n",
        "        # extracting from dictionary \n",
        "        data = D['image']\n",
        "        target = D['landmarks']\n",
        "        #print(type(target))\n",
        "        \n",
        "        #target = target[0]\n",
        "        target = target.view(-1, 1)\n",
        "        \n",
        "\n",
        "        # formatting and modifying output from dict\n",
        "        \n",
        "        target = torch.tensor(target).float()\n",
        "        #data = data.float()\n",
        "                \n",
        "        #### TRAINING PROPER ####\n",
        "        #########################\n",
        "        \n",
        "        #print(\"Epoch:\", epoch + 1, \"Iteration:\", iter + 1, \"out of:\", n_iterations)\n",
        "        \n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward pass: compute predicted outputs by passing inputs to the model     \n",
        "        outputs = soil_model(data)\n",
        "        #print(outputs)\n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, target)\n",
        "        \n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "      \n",
        "    #  scheduler - perform a its step in here - controls rate of learning\n",
        "    scheduler.step()\n",
        "    \n",
        "    # print training statistics - calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(tl_soil.dataset)\n",
        "    train_loss_progress.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "\n",
        "\n",
        "\n",
        "    # define variables\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    #prep for evaluation\n",
        "    soil_model.eval() \n",
        "\n",
        "    with torch.no_grad(): #not exactly sure what this does\n",
        "        for iter, D in enumerate(vl_rock):\n",
        "                        \n",
        "\n",
        "            \n",
        "            # extracting from dictionary \n",
        "            data = D['image']\n",
        "            target = D['landmarks']\n",
        "            \n",
        "            # test prints\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # formatting data from the dict\n",
        "            target = torch.tensor(target)\n",
        "            \n",
        "            \n",
        "            # VALIDATION PROPER\n",
        "            \n",
        "            # returns the output if < 1, else 1 - converts output to probability\n",
        "            outputs = soil_model(data)\n",
        "            \n",
        "            #print(outputs)\n",
        "            predicted, sp = torch.max(outputs.data, 1)\n",
        "            \n",
        "            presize = predicted.size()\n",
        "            psize = list(presize)[0]\n",
        "            \n",
        "            for index in range(psize):\n",
        "                entry = predicted[index]\n",
        "                value = entry.item()\n",
        "                if (value < 0.5):\n",
        "                    value = 0.0\n",
        "                else:\n",
        "                    value = 1.0\n",
        "                predicted[index] = torch.tensor(value)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # does the addition\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "        validation_accuracy_progress.append(100* correct/total)\n",
        "\n",
        "\n",
        "    print('Accuracy of the network on the validation set: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "\n",
        "torch.save(soil_model, PATH)\n",
        "\n",
        "traintext.close()\n",
        "valtext.close()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LyIjiZeh2UC"
      },
      "source": [
        "x_range = np.arange(1, n_epochs+1)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(x_range, train_loss_progress, c='b', label=\"Train loss\")\n",
        "axs[1].plot(x_range, validation_accuracy_progress, c='r', label=\"Test accuracy\")\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}